{"2024-09-23":{"title":"2024-09-23","links":[],"tags":[],"content":"location: San Francisco, CA\nunits: Fahrenheit"},"Advance-Energy-Systems/Dyson-Spheres":{"title":"Dyson Spheres","links":[],"tags":[],"content":""},"Advance-Energy-Systems/Fission-Reaction":{"title":"Fission Reaction","links":[],"tags":[],"content":"Lets take U-235 as an example. U-235 has 92 protons and 143 neutrosns in its atomic nucleus. When neutro collides with a U-235 atom, it can cause the nuclear to split (fission) if energy of the neutron is sufficient"},"Advance-Energy-Systems/Neutron-Scattering":{"title":"Neutron Scattering","links":["Advance-Energy-Systems/moderator"],"tags":[],"content":"\nNeutrons released from the fission reaction are moving at very high speeds typically millions of kilometers per hour\nMaterials called a ‚Äúmoderator‚Äù are used to to slow down the neutrons to allow them to interact more efficient with the fuel atoms\n"},"Advance-Energy-Systems/Nuclear-Power-Plants":{"title":"Nuclear Power Plants","links":["Advance-Energy-Systems/control-rods","Advance-Energy-Systems/Neutron-Scattering","Advance-Energy-Systems/Fission-Reaction","Advance-Energy-Systems/Nuclear-Reactor","Advance-Energy-Systems/uranium-enrichment"],"tags":[],"content":"Overview\nA nuclear power plant generates electricity by harnessing the energy released from\nthe splitting (fission) of atomic nuclei. This process is called nuclear reaction,\nand it occurs in a reactor.\nReactor Basics\nThe reactor core is the heart of a nuclear power plant. It‚Äôs where the fission\nreactions take place. The core consists of:\n\nFuel: Typically, uranium-235 (U-235) or other fissile materials like\nplutonium-239 (Pu-239). The fuel is in the form of small pellets or rods.\nModerator: A material that helps slow down neutrons, increasing the chances\nof fission reactions. Water, heavy water, or graphite are common moderators.\nControl Rods: Made of neutron-absorbing materials like boron or cadmium,\nthese rods regulate the reaction by absorbing excess neutrons.\n\nHow Fission Reactions Work\nHere‚Äôs a simplified explanation:\n\nA neutron collides with a fuel atom (e.g., U-235), causing it to split\n(fission) and release more neutrons\nThese new neutrons then collide with other nearby fuel atoms, triggering\nadditional fission reactions.\nThe process continues, releasing even more neutrons and energy.\n\nNeutron Interactions:\n\nAbsorption: Neutrons are absorbed by control rods or other materials,\nreducing the reaction rate.\nNeutron Scattering: Neutrons collide with surrounding atoms, changing direction but\nnot releasing much energy.\nFission Reaction: As mentioned earlier, neutrons cause fuel atoms to split,\nreleasing more neutrons and energy.\n\nHere‚Äôs a step-by-step breakdown of the process:\n\nNeutron emission: A neutron is released from the fission reaction.\nScattering: The neutron collides with a moderator atom (e.g., hydrogen or\ncarbon), slowing down its speed.\nAbsorption: The slowed-down neutron collides with a U-235 atom, absorbing\nits energy and causing it to split (fission).\nFission: The U-235 nucleus splits, releasing more neutrons and energy.\n\nIn-Pile Fuel\nTo increase the chances of neutron collisions with fuel atoms, nuclear reactors\noften use in-pile fuel designs. In these designs, the fuel rods are arranged in a\nway that allows the neutrons to have multiple scattering events before they\ninteract with the fuel atoms. This increases the reaction rate and overall power\noutput.\nHeat Transfer\nAs fission reactions release energy, it‚Äôs converted into heat (thermal energy).\nThis heat is transferred to a coolant, such as:\n\nWater: Used in pressurized water reactors (PWRs) or boiling water reactors\n(BWRs).\nGas: Employed in gas-cooled reactors (GCRs).\n\nSteam Generation\nThe heated coolant transfers its energy to a steam generator, producing\nhigh-pressure steam.\nTurbine and Generator\nThe steam drives a turbine, which is connected to an electrical generator. As the\nturbine spins, it converts the mechanical energy into electrical energy (AC\npower).\nCooling Systems\nTo prevent overheating, nuclear power plants use various cooling systems:\n\nPrimary Cooling: The coolant loop that transfers heat from the reactor.\nSecondary Cooling: A separate loop that removes excess heat and sends it to\na condenser or heat exchanger.\n\nSafety Features\nNuclear power plants have numerous safety features to prevent accidents and\nmitigate their consequences:\n\nContainment Building: A reinforced structure surrounding the reactor,\ndesigned to prevent radioactive releases.\nEmergency Core Cooling Systems (ECCS): Automatic systems that inject\ncoolant into the reactor in case of an emergency.\nControl Rods: As mentioned earlier, these rods regulate the reaction and\ncan be used to quickly shut down the reactor.\n\n\nWhere to Start:\nThe United States Nuclear Regulatory Commission (NRC) is responsible for regulating nuclear power plants.\nFactors to consider:\n\nAccessibility: Proximity to major highways, railways, and ports.\nEnvironmental Concerns: Areas with minimal environmental impact, such as avoiding sensitive\necosystems or wildlife habitats.\nPublic Perception: Regions with a relatively low risk of public opposition or concerns.\n\nSome potential areas in the United States to look into\n\nSavannah River Site, Georgia: A former nuclear weapons production site that‚Äôs been repurposed\nfor research and development.\nHanford Nuclear Reservation, California: A decommissioned nuclear site that could be\nrevitalized.\nOak Ridge, Tennessee: Home to the Oak Ridge National Laboratory, this area has a strong\nnuclear industry presence.\n\nInitial Steps:\nTo start building your nuclear power plant, follow these steps:\n\nSite Selection and Acquisition: Identify a suitable location and acquire the necessary land\nor property rights.\nConceptual Design: Develop a preliminary design for your plant, including reactor type, fuel\ncycle, and waste management strategies.\nPre-Application Phase: Engage with stakeholders, conduct environmental assessments, and\ndevelop a community engagement plan.\n\nLicensing Process:\nThe NRC will guide you through the licensing process:\n\nCombined License (COL): Apply for a COL to build and operate your nuclear power plant.\nConstruction Permit (CP): Obtain a CP from the NRC before commencing construction.\nOperating License (OL): After construction is complete, apply for an OLP to begin operating\nyour plant.\n\nKey Licenses and Permits:\n\nNuclear Regulatory Commission (NRC) Licenses: COL, CP, and OLP.\nEnvironmental Protection Agency (EPA) Permits: National Environmental Policy Act (NEPA),\nEndangered Species Act (ESA), and Clean Water Act permits.\nState and Local Permits: Obtain necessary approvals from state and local authorities for\nconstruction and operation.\n\nWhat to Build:\n\nNuclear Reactor: Choose a reactor design, such as pressurized water reactors (PWRs) or\nboiling water reactors (BWRs).\nFuel Cycle: Develop a fuel cycle strategy, including fuel fabrication, irradiation, and\nreprocessing.\nWaste Management: Design a waste management system for managing spent nuclear fuel and other\nradioactive materials.\nCooling System: Install a cooling system to dissipate heat generated by the reactor.\n\nSoftware for Control Systems:\nYou‚Äôll need software for controlling and monitoring your plant‚Äôs systems:\n\nReactor Control System (RCS): Software for controlling the reactor‚Äôs power output, coolant\nflow, and other critical parameters.\nSafety Instrumentation and Control Systems: Software for monitoring and controlling\nsafety-related parameters, such as radiation levels and containment integrity.\nPlant Control System: Software for managing overall plant operations, including generating\nelectricity and managing waste.\n\nTransmission Line Connection:\nTo connect your nuclear power plant to the grid:\n\nInterconnection Agreement: Negotiate with regional transmission organizations (RTOs) or\nindependent system operators (ISOs) to establish a transmission line connection.\nGrid Study: Conduct a grid study to determine the best possible point of interconnection and\nensure stability and reliability.\n\nBenefits\n\nEnergy density: Nuclear power plants can generate a significant amount of electricity per unit of fuel consumed (around 10-15 MWt/Mtonne UO2). This makes them more energy-dense than many renewable sources, such as solar or wind.\nScalability: Nuclear reactors can be designed to be relatively large and scalable, which allows for the potential to generate a significant amount of electricity from a single facility. For example, a typical nuclear reactor can produce around 1-2 gigawatts (GW) of electricity.\nBaseload power: As I mentioned earlier, nuclear power plants can provide reliable, constant power (baseload), which is essential for supporting the grid and ensuring a stable supply of is electricity.\n\nEnrichment to raw energy output\nuranium enrichment\n\nEnrichment process energy input: The enrichment process requires significant energy inputs to\noperate the facilities, such as:\n\nElectricity for powering centrifuges or other enrichment technologies\nCooling systems for maintaining equipment temperatures\nHeating and ventilation systems for maintaining a safe working environment\n\n\nEnergy output from enriched uranium: When you produce enriched uranium (e.g., U-235), it can be\nused to generate electricity in nuclear power plants. The energy released from the fission reaction\nis converted into heat, which drives turbines to produce electricity.\nComparison of energy inputs and outputs:\n\nEnergy input (enrichment process): Typically around 0.1-0.3 GJ/tU (gigajoules per tonne of\nuranium)\nEnergy output (electricity generation): For a typical nuclear power plant, around 10-15 GW¬∑h/MWh\n(gigawatts-hour per megawatt-hour)\n\n\n\nNow, let‚Äôs calculate the net energy gain:\n\nNet energy gain: Assuming an average enrichment factor of 3.5% (from natural uranium to\nreactor-grade fuel), and using the above energy inputs and outputs:\n\nEnergy input: 0.2 GJ/tU (average of 0.1-0.3 GJ/tU)\nEnriched uranium produced per tonne of natural uranium: approximately 35 kg U-235 (from 700 kg\nnatural uranium)\nEnergy output from enriched uranium: around 10-15 GW¬∑h/MWh, assuming an average electricity\nproduction efficiency of 33% (common for nuclear power plants)\n\n\n\nUsing these values, we can estimate the net energy gain:\n\nNet energy gain: Approximately 3.5 GJ/tU (gigajoules per tonne of uranium), or around 17-20\nGW¬∑h/MWh\n\nSo, in terms of energy output vs. enrichment process energy input, it appears that enriching uranium\nto produce reactor-grade fuel is a net positive outcome.\nKeep in mind that these calculations are simplified and do not account for various factors, such as:\n\nEnergy losses during processing: Some energy is lost during the enrichment process due to\nequipment inefficiencies, heat generation, and other factors.\nInfrastructure and operational costs: Building and operating enrichment facilities, as well as\nthe nuclear power plants themselves, require significant investments in infrastructure, personnel,\nand maintenance.\nEnvironmental and social considerations: The nuclear industry has a complex environmental and\nsocial footprint that must be carefully managed to ensure responsible energy production.\n"},"Advance-Energy-Systems/Nuclear-Reactor":{"title":"Nuclear Reactor","links":["Pressurized-Water-Reactors","Boiling-Water-Reactors","Gas-cooled-Reactors","Liquid-metal-Fast-Breeder-Reactors"],"tags":[],"content":"\nPressurized Water Reactors (PWRs):\nReactor core is surrounded by pressurized water (coolant) that helps regulate the reaction. Fuel rods are typically made of enriched uranium (U-235) or other fissile materials.\nBoiling Water Reactors (BWRs):\nBWR uses a boiling coolant instead of pressurized water. This is simpler and less expensive to build than a PWR, but it‚Äôs not as efficient in terms of energy production.\nGas-cooled Reactors\nReactor core is surrounded by a gas (usually carbon dioxide or helium) that helps regulate the reaction. The fuel rods are typically made of enriched U-235 or other fissile materials.\nLiquid-metal Fast Breeder Reactors (LMFBRs):\nAn LMFBR uses liquid metal (typically sodium or lead) as a coolant and moderator instead of water. This design is capable of producing more energy than traditional PWRs.\n\nDesigns:\nReactor designs can vary depending on the specific application, location, and desired performance characteristics. Some common reactor designs include:\n\nPile Reactors: A simple design where fuel rods are stacked vertically to form a ‚Äúpile‚Äù\nstructure.\nLattice Reactors: A more complex design where fuel rods are arranged in a lattice pattern to\nimprove neutron interactions.\nPlasma Reactors: A high-temperature design that uses plasma (ionized gas) as the coolant and moderator instead of water.\n"},"Advance-Energy-Systems/The-Grid":{"title":"The Grid","links":[],"tags":[],"content":"Introduction\n\n\nCoal ‚Üí carbon emissions\n\n\nNatural gas ‚Üí methan emissions\n\n\nNuclear ‚Üí poisoned waste\n\n\nOil ‚Üí cost in wartime blood\n\n\nWhat is the grid?\nIn America, there is three of them:\n\n\none for the West and a bit of Mexico and much of western Canada\n\n\none for all of the East\n\n\nsmaller one for Texas\n\n\nFor the most part Mexico has its own grid, but Canada does not (except for its errant province, Quebec, which, like Texas, has chosen to keep its options for secession open by managing its infrastructure for itself).\nBlackouts\n\n\n2003 East Coast blackout\n\n\ncaused by overgrown tree and a computer bug blacked out weight states and 50 million\n\n\n6 billion dollars lost in GDP\n\n\n\n\nProblems\n\n\nBoth how renewables make power but where they make it\n\n\nWind farms go up where it is windy\n\nWyoming or Iowa or West Texas good options but they don‚Äôt have very good lon-distance power lines to carry it to more promising markets Natural gas\n\n\n\nMethane emissions from natural gas development and leaky transport infrastructure\n\n\nundercut some of the climate advantage gas has over coal.\n\n\nOne 2012 study estimated that if the industry were to let leak only 3.2 percent of the gas produced, it could be worse for the climate than coal.\n\n"},"Advance-Energy-Systems/control-rods":{"title":"control rods","links":[],"tags":[],"content":"\nMade of materials that absorb neutrons such as boron or cadmium\nwhen it is inserted into the reactor core, it absorbs excess neutrons reduce the reaction rate\nthis allows operators regulate the power output of the reactor\n"},"Advance-Energy-Systems/gaseous-diffusion":{"title":"gaseous diffusion","links":[],"tags":[],"content":""},"Advance-Energy-Systems/index":{"title":"Advance Energy Systems","links":["Advance-Energy-Systems/Nuclear-Reactor","Advance-Energy-Systems/Dyson-Spheres"],"tags":[],"content":"\nNuclear Reactor\nDyson Spheres\n\n"},"Advance-Energy-Systems/moderator":{"title":"moderator","links":[],"tags":[],"content":"For example colliding with hydrogen or carbon, it slows down its speed"},"Advance-Energy-Systems/uranium-enrichment":{"title":"uranium enrichment","links":["Advance-Energy-Systems/gaseous-diffusion"],"tags":[],"content":"Raw uranium (also known as yellowcake) is primarily found in sedimentary rock formations, often\nassociated with other minerals such as copper, gold, and silver. Here are some of the top\nuranium-producing countries and regions:\n\nAustralia: Australia is the world‚Äôs largest uranium producer, accounting for around 20% of\nglobal production. Major mines include:\n\nOlympic Dam (South Australia)\nRanger Uranium Mine (Northern Territory)\n\n\nCanada: Canada is a significant uranium producer, with mines located in:\n\nSaskatchewan (McArthur River and McClean Lake mines)\nQuebec (Strathcona and Malartic mines)\n\n\nNevada (USA): The United States has several uranium-producing states, including:\n\nNevada: The state‚Äôs largest uranium mine is the Smith Valley Mine\n\n\nUruguay: Uruguay has significant uranium deposits, with major mines located in:\n\nPunta de Rieles (Salto and San Mateo mines)\n\n\nRussia: Russia has a long history of uranium mining, with notable deposits found in:\n\nKazakhstan (Kyzylorda region)\nSiberia (Lake Baikal area)\n\n\nNamibia: Namibia is an emerging uranium-producing country, with significant deposits found\nin:\n\nR√∂ssing mine\n\n\nUzbekistan: Uzbekistan has a number of uranium mines, including:\n\nAlmalyk mine\n\n\n\nMethods:\n\ngaseous diffusion: gas containing uranium hexafluroide (UF6) is passed through a series of porous membranes to separate out the lighter and heavier atoms.\nLiquid-liquid centrifugation: uses two different liquids to separate the isotopes based on their activity\n"},"Biology/The-Genetic-Code":{"title":"The Genetic Code","links":[],"tags":[],"content":"Central Dogma\nSandra: ‚ÄúGot that central dogma in me‚Äù\nDNA Structure\n\n\nSingle Circular Chromosome (prokaryotic DNA structure, typically seen in bacteria and archaea)\n\n\nIn contrast to eukaryotic cells (which have multiple linear chromosomes), prokaryotes feature a more streamlined and efficient genome architecture with fewer regulatory elements.\n\n\n\nHorizontal Gene Transfer (HGT)\n\nThe process by which organisms transfer genetic material between one another, outside of traditional reproduction (vertical transmission).\nPlays a critical role in bacterial evolution, contributing to the spread of antibiotic resistance, virulence factors, and metabolic pathways.\n\n1. Vertical Transmission\n\nGenes are passed from parent to offspring through standard reproduction.\n\n2. Horizontal Transmission\n\nGenes are transferred between organisms, potentially across species barriers.\n\nMechanisms of Horizontal Transmission:\n\n\nConjugation: Transfer via sex pilus\n\n[GOOD] for bacteria\nA direct cell-to-cell contact is established, allowing the exchange of plasmids (extrachromosomal DNA).\nF-plasmid: Contains genes necessary for the formation of the sex pilus.\nImportant for spreading antibiotic resistance genes.\n\n\n\nTransduction: Transfer via bacteriophages (viruses that infect bacteria)\n\n[BAD] for bacteria\nBacteriophages can accidentally package bacterial DNA and transfer it to other bacteria during infection.\nThere are two types:\n\nGeneralized Transduction: Random bacterial DNA is packaged into the viral particle.\nSpecialized Transduction: Only specific bacterial genes adjacent to the viral integration site are transferred.\n\n\nThis can introduce foreign DNA that disrupts bacterial function or integrates into the host genome, potentially altering behavior or killing the bacteria.\n\n\n\nTransformation: Uptake of naked DNA (often plasmids) from the environment\n\n[GOOD] for bacteria\nBacteria take up DNA fragments or plasmids from their surroundings and incorporate them into their genome.\nNatural competence (ability to uptake DNA) varies across species. In lab settings, methods to induce competence include:\n\nHeat shock: Involves exposing bacteria to cold followed by rapid warming, opening pores in the membrane to allow DNA uptake.\nElectroporation: Uses electric pulses to create temporary pores in the membrane for plasmid insertion.\n\n\nApplication: Transformation is widely used in genetic engineering to introduce desired traits or genes into bacterial hosts.\n\n\n\n\nEvolutionary Conundrum\n\nHorizontal gene transfer challenges the traditional view of evolution as a tree-like structure of descent from a common ancestor. Instead, HGT presents a more networked model of evolution with genes shared between unrelated organisms.\nHGT is particularly prominent in prokaryotes, allowing rapid adaptation to changing environments (e.g., antibiotic pressure). This complicates the evolutionary tree and forces reconsideration of how traits spread across populations.\n\n\nThe Restriction-Modification (RM) System\nThe Immune System of Bacteria\n\nBacteria have evolved sophisticated systems to defend themselves against foreign DNA, particularly from bacteriophages (viruses that infect bacteria).\n\nComponents:\n\n\nRestriction Enzymes (REase)\n\nThese enzymes cut DNA at specific recognition sites (motifs) that are usually palindromic (e.g., 5‚Äô-GAATTC-3‚Äô).\nThey serve as a defense mechanism by cleaving foreign DNA (such as bacteriophage DNA), preventing infection.\nMany restriction enzymes are named after the bacteria from which they were isolated (e.g., EcoRI from E. coli).\n\n\n\nMethyltransferases (MTase)\n\nMethylate the host‚Äôs own DNA at the same recognition sites targeted by restriction enzymes.\nMethylation acts as a protective ‚Äúmark,‚Äù preventing the host‚Äôs restriction enzymes from cutting its own DNA.\nThis system of paired restriction enzymes and methyltransferases ensures foreign DNA is destroyed while the host genome remains intact.\n\n\n\n\nStealth-by-Engineering\n\n\nSome engineered bacterial strains can evade detection by restriction enzymes through a process termed ‚ÄúStealth-by-Engineering.‚Äù\n\n\nThis is particularly useful in synthetic biology, where scientists want to insert foreign DNA into bacteria without triggering the RM system.\n\n\nMethods to evade RM systems include:\n\nMethylation of foreign DNA: Before introducing the DNA into bacteria, scientists can methylate it to mimic the host‚Äôs natural DNA.\nSynthetic redesign of DNA motifs: Using advanced DNA synthesis platforms (such as PacBio and Twist Bioscience), specific restriction sites are modified or removed, preventing cleavage by restriction enzymes. This allows for safe integration of engineered genes into bacterial genomes.\nCodon optimization: In stealth engineering, scientists optimize codon usage (changing DNA sequences without altering the protein) to make the inserted genes resemble the host‚Äôs native DNA.\n\n\n\nInvisible Engineering:\n\nThis concept involves using genetic engineering techniques to modify bacteria in ways that avoid detection by their immune-like RM systems.\nBy manipulating methylation patterns and restriction enzyme recognition sites, researchers can ‚Äúcloak‚Äù foreign DNA, rendering it invisible to bacterial defenses.\nThis has significant applications in biotechnology, including gene therapy, vaccine development, and the production of pharmaceuticals.\n\nExample: Engineered bacterial strains designed to produce therapeutic compounds or break down pollutants without triggering their own RM defenses.\n\n\n\nDNOVO Synthesis\n\nDNOVO refers to de novo (from scratch) DNA synthesis, enabling precise construction of genetic sequences without the need for a template.\nTwist Bioscience and other companies offer custom DNA synthesis services, allowing scientists to design and create novel genes, pathways, or even entire genomes tailored to specific functions.\n\nApplications:\n\nProduction of synthetic biology products: biofuels, pharmaceuticals, and enzymes.\nCreation of genetically modified organisms (GMOs) with improved traits (e.g., drought resistance in plants).\nDevelopment of personalized medicine: custom therapies based on individual genetic profiles.\n\n\n\n\n\n\nInvisible Engineering and the Future\n\nInvisible engineering is part of a broader trend in synthetic biology, where organisms are modified in subtle ways to achieve desired outcomes while avoiding detection by natural systems (such as immune responses or environmental stressors).\nAs technologies like CRISPR-Cas9 and long-read sequencing (PacBio) continue to improve, the potential for more sophisticated and stealthy genetic modifications grows.\nEthical concerns arise as the ability to engineer life forms becomes more advanced, raising questions about how far humanity should go in manipulating the genetic code.\n\nKey Future Areas:\n\nGene Therapy: Using engineered viruses to introduce therapeutic genes into human cells without triggering immune rejection.\nBioremediation: Engineering bacteria that can detoxify pollutants without being attacked by native bacterial communities.\nAgriculture: Developing crop plants with enhanced traits (yield, resistance to pests) that do not suffer from horizontal gene transfer risks or environmental interference.\n\n\nThis extended version includes additional details and context, especially about invisible engineering and its applications in synthetic biology. It also integrates relevant examples to make the content clearer and more insightful."},"Biology/index":{"title":"index","links":[],"tags":[],"content":""},"CUDA/3-component-vector-that-represent-the-thread-index-within-a-block-1":{"title":"3-component vector that represent the thread index within a block 1","links":[],"tags":[],"content":""},"CUDA/CUDA":{"title":"CUDA","links":["CUDA/3-component-vector-that-represent-the-thread-index-within-a-block-1"],"tags":[],"content":"docs.nvidia.com/cuda/cuda-c-programming-guide/\nDefinition\n\n\nhost: the CPU\n\n\ndevice: the GPU\n\n\nhost memory: the system main memory\n\n\ndevice memory: onboard memory on a GPU card\n\n\nkernels: a GPU function launched by the host and executed on the device\n\n\ndevice function: a GPU function executed on the device which can only be called from the device (i.e. from a kernel or another device function)\n\n\n1. Introduction\n\n\nGPU Applicationwww.nvidia.com/en-us/gpu-accelerated-applications/\n\n\nFPGAs also energy efficient but don‚Äôt offer much programming flexibility\n\n\nGPU design for highly parallel computation. More transistors are devote to data processing.\n\n\n2. CUDA: General Purpose Parallel Computing Platform and Programming Model\nCUDA comes with a software environment that can use C++. Also available are FORTRAN, DirectCompute, OpenACC.\n3. Scalable Programming Model\n\n\nCUDA parallel programming is design to overcome parallelism problems for GPUs\n\n\nThree key abstractions:\n\n\nhierarchy of thread groups\n\n\nshared memories\n\n\nbarrier synchronization\n\n\n\n\nAutomatic scalability\n\n\nGPU is built around an array of Streaming Multiprocessors (SMs)\n\n\n4. Document Structure\njust describes document structure\n5. Programming Model\nFull code for the vector addition example\nKernels\nCUDA C++ extends C++ by allowing programmer to define C++ functions called kernels, that when called, are executed N times in parallel by N different CUDA threads. This is opposed to only once like regular C++ functions.\n\n\nKernel is defined using __global__ declaration specifier and the number of CUDA threads that kernel for a given kernel call is specified using a new &lt;&lt;&lt;...&gt;&gt;&gt; execution configuration syntax.\n\n\nEach thread that executes the kernel is given a unique thread ID that is accessible within the kernel through built-in variables\n\n\nex. As an illustration, the following sample code, using the built-in variable threadIdx, adds two vectors A and B of size N and stores the result into vector C:\n‚àï‚àï Kernel definition\n__global__ void VecAdd(float* A, float* B, float* C) {\n\n  int i = threadIdx.x;\n\n  C[i] = A[i] + B[i];\n}\n\nint main() { ...\n\n‚àï‚àï Kernel invocation with N threads VecAdd&lt;&lt;&lt;1, N&gt;&gt;&gt;(A, B, C); ...\n\n}\n\nHere, each of the N threads that execute VecAdd(0 performs one pair-wise addition\nThread Hierarchy\n\n\nA ‚Äúthread block‚Äù refers to a group of threads that are executed simultaneously on the same processor or core. In parallel computing, threads are organized into blocks based on their access patterns and spatial locality. For example, in a vector processing algorithm, threads might be grouped into blocks based on the index of the vector element they are operating on.\n\n\nThe relationship between a thread‚Äôs index and its thread ID is straightforward:\n\n\nFor a one-dimensional block, the thread ID is simply the same as the thread index.\n\n\nFor a two-dimensional block of size (Dx, Dy), the thread ID of a thread of index (x, y) is (x + y Dx).\n\n\nFor a three-dimensional block of size (Dx, Dy, Dz), the thread ID of a thread of index (x, y, z) is (x + y Dx + z Dx Dy).\n\n\n\n\nIn each case, the thread ID is calculated by adding the component of the index that corresponds to the dimension of the block in which the thread is executing. This allows threads to be addressed and communicated with within a block, as well as across blocks.\n\n\nWhat does it mean for a block to be a one-dimensional block? In the context of parallel computing, a ‚Äúone-dimensional block‚Äù refers to a block of threads where all threads are executed in a single dimension. In other words, all threads within the block have the same index in that dimension.\nFor example, consider a vector processing algorithm that operates on vectors of length N. In this case, each thread can be assigned an index from 0 to N-1, and all threads within a block will have the same index in the same dimension (i.e., all threads within the block are executing the same operation on the same position of the vector).\nIn this scenario, the thread ID calculation would be simple: for any thread within the block, its thread ID is simply its index within the block. So, for example, if a thread with index 2 is within a one-dimensional block of size 5, then its thread ID is 2.\nOne-dimensional blocks are useful when the access pattern and spatial locality of the threads are well-defined in a single dimension. This allows for efficient parallelization and coordination within the block, as all threads can be addressed and communicated with using a simple index.\nthreadIdx\n3-component vector that represent the thread index within a block 1. Makes it natural to represent a vector, matrix, and volume.\nIn the CUDA programming model, threadIdx is a 3-component vector that represents the thread index within a block. The three components are:\n\n\nthreadIdx.x: The index of the thread in the one-dimensional dimension of the block. For a one-dimensional block, this is the same as the thread ID.\n\n\nthreadIdx.y: The index of the thread in the two-dimensional dimension of the block (if the block size is greater than 1).\n\n\nthreadIdx.z: The index of the thread in the three-dimensional dimension of the block (if the block size is greater than 2).\n\n\nThe relationship between threadIdx and the thread ID is straightforward: For a one-dimensional block, threadIdx and threadID are the same; for a two-dimensional block of size (Dx, Dy), threadIdx.y represents the thread ID of a thread of index (x, y) as (x + y * Dx). For a three-dimensional block of size (Dx, Dy, Dz), threadIdx.z represents the thread ID of a thread of index (x, y, z) as (x + y * Dx + z * Dx * Dy).\nThe threadIdx vector is used to identify threads within a block, and it is used in conjunction with the blockDim vector to access threads within a block. The blockDim vector represents the size of the block in each dimension, and it is used to calculate the index of a thread within the block.\nThe limit on the number of threads per block is due to the fact that all threads within a block must share the limited memory resources of the same streaming multiprocessor core. On current GPUs, a thread block may contain up to 1024 threads. However, multiple equally-shaped thread blocks can be executed simultaneously, allowing the total number of threads to be equal to the number of threads per block in each dimension.\nCooperative groups are used to enable efficient cooperation among threads within a block. Shared memory is expected to be a low-latency memory near each processor core (much like an L1 cache), and __syncthreads() is expected to be lightweight. Cooperative groups provide a rich set of thread-synchronization primitives that can be used to coordinate memory accesses among threads within a block.\nStreaming multiprocessor core\nThread block clusters\nProgramming Interface\nCompilation with nvcc\n\n\nKernels can be written using the CUDA instruction set architecture called PTX.\n\n\nMore effective to use high-level programming language such as C++\n\n\nBoth cases, kernels must be compiled into binary code by nvcc to execute on device\n\n\nnvcc\n\ncompiler drive to simplify process of compiling C++ or PTX code\n\nLink [[nvcc user manual]]\nCompilation workflow\n\nOffline compilation\n\n\n\nnvcc basic workflow consists of separate GPU code from CPU code and then:\n\n\nCompiling GPU code into PTX code and/or binary form ([[cubin]] object)\n\n\nmodifying host code by replacing the &lt;&lt;&lt;...&gt;&gt;&gt; by necessary CUDA runtime function calls to load and launch each compiled kernel from the PTX code and/or cubin object\n\n\n\n\nThe modified host code is output either as C++ code that is left to be compiled using another tool or as object code directly by letting nvcc invoke the host compiler during the last compilation stage (NOTE: can this be made efficient?)\nApplications can then:\n\n\neither link to the compiled host code (this is the most common use case)\n\n\nOr ignore the modified host code (if any) and use the CUDA driver API to load and execute the PTX code or cubin object\n\n\n\nJust-in-Time Compilation\n\nJust-in-time (JIT) compilation is a technique where code is compiled and executed on the fly, at the time it‚Äôs needed, rather than being compiled and loaded into memory all at once. In the context of CUDA, JIT compilation is used to compile C++ device code into PTX binary code that can be executed directly on the GPU.\nHere‚Äôs how it works:\n\n\nWhen an application is launched, the device driver compiles any PTX code loaded by the application at runtime using a just-in-time compiler. This increases the load time of the application, but allows it to benefit from any new compiler improvements that come with each new device driver release.\n\n\nThe compiled binary code is cached by the device driver for subsequent invocations of the application to avoid repeating the compilation process. The cache is automatically invalidated when the device driver is upgraded, so applications can benefit from the improvements in the new just-in-time compiler built into the device driver.\n\n\nEnvironment variables can be used to control JIT compilation, as described in CUDA Environment Variables.\n\n\nAs an alternative to using nvcc to compile CUDA C++ device code, NVRTC (NVIDIA Runtime Compilation) can be used to compile CUDA C++ device code to PTX at runtime. NVRTC is a runtime compilation library for CUDA C++; more information can be found in the NVRTC User guide.\n\n\nExamples of JIT compilation in action:\n\n\nImagine an application that needs to perform a computationally intensive task, such as matrix multiplication. Instead of loading the entire code for the matrix multiplication function at once, the device driver can compile just-in-time the necessary parts of the function when the application invokes it. This reduces the load time and memory usage, while still allowing the application to benefit from any optimizations that come with each new device driver release.\n\n\nAnother example is a web browser that needs to render 3D graphics for a web page. Instead of loading the entire 3D rendering engine at once, the device driver can compile just-in-time the necessary parts of the engine when the browser invokes them. This reduces the load time and memory usage, while still allowing the browser to benefit from any optimizations that come with each new device driver release.\n\n\nIn summary, JIT compilation in CUDA allows applications to benefit from improved compilers and other optimizations without requiring a complete code recompilation when the device driver changes. It does this by compiling only the necessary parts of the application at runtime, caching the results for subsequent invocations, and invalidating the cache when the device driver is upgraded.\n\nWhat does ‚Äúnew device driver changes‚Äù mean?\n\nWhen we talk about ‚Äúnew device driver changes,‚Äù we‚Äôre referring to updates or improvements made to the device driver that runs on the GPU. The device driver is responsible for managing the communication between the CPU and the GPU, and it provides a layer of abstraction between the two. As new hardware technologies are developed, the device driver may be updated to take advantage of these advancements. These updates could include improved support for parallel processing, faster data transfer rates, or new features that allow developers to create more complex and powerful applications.\nWhen a new device driver is released, it may introduce changes to the way code is compiled and executed on the GPU. For example, the new driver might provide better support for a particular programming model or feature, or it might introduce new APIs or abstractions that simplify development. In either case, these changes can affect how code is compiled and executed, and JIT compilation helps ensure that applications can take advantage of these improvements without requiring a complete code recompilation.\n\nDoes this mean that code won‚Äôt be compiled unless the JIT thinks it‚Äôll be needed?\n\nNot exactly. JIT compilation is designed to optimize code execution based on the specific needs of the application and the hardware platform it‚Äôs running on. When an application loads PTX code, the device driver will perform just-in-time compilation only if necessary. This means that not all PTX code will be compiled at once; instead, the JIT compiler will only compile the parts of the code that are actually needed by the application.\nFor example, imagine an application that performs a matrix multiplication operation. If the application only needs to perform this operation a few times, then the JIT compiler won‚Äôt compile the entire matrix multiplication function at once. Instead, it will only compile the specific parts of the function that are needed for those few invocations. This can reduce the load time and memory usage for the application, while still allowing it to benefit from any optimizations that come with each new device driver release.\nIn summary, JIT compilation is a dynamic optimization technique that helps optimize code execution based on the specific needs of the application and the hardware platform it‚Äôs running on. It doesn‚Äôt mean that code won‚Äôt be compiled unless the JIT thinks it‚Äôll be needed; instead, it means that the JIT will only compile the necessary parts of the code at runtime to optimize performance.\nBinary Compatibility\n[[Compute capability seems to be a big thing]]\nBinary code is architecture specific. A cubin object is generated using the compiler option -code that specifies the targeted architecture.\nPTX Compatibility\nWarp Shuffle Functions\nApplication Compatibility\nTo execute code on devices of specific compute capability, an application must load binary or PTX code that is compatible with this compute capability.\nReview this section with access to the [[nvcc user manual]]\nCUDA runtime\nRuntime is implemented in the cudart library which is linked to the application either statically via cudart.lib or libcudart.a or dynamically via cudart.dll or lidcudart.so.\n1. Initialization"},"CUDA/index":{"title":"CUDA","links":[],"tags":[],"content":""},"Computer-Architecture/AC":{"title":"AC","links":[],"tags":[],"content":"source: www.youtube.com/watch\nAlternating current\n\nAC electricity - electron alternates by flowing forward and backwards constantly\nWhat is frequency?\nRefers to how many times the AC sine wave repeats per second\nin North America it‚Äôs 60 Hz of electricity. Sine wave repeats 60 times per second\nPolarity changes 120 times per second (+ to -\nRest of the world it‚Äôs 50 Hz\nDifferent phases\n\nSingle phase\n\nJust one phase from the generator\nlarge gaps between phase\n\n\nThree phase\n\nHave a connection to each of the three phases\n\nWith this the distance between the peaks decrease and thus you can have more power\n\n\nSplit phase\n"},"Computer-Architecture/ARM-architecture":{"title":"ARM architecture","links":["Analog-circuits","Digital--circuits","Microarchitecture","architecture","Buffer","Supply-voltage","Logic-Levels","Noise-margins","Computer-Architecture/inverter","Computer-Architecture/CMOS","Transistors","Bubble-Pushing","Karnaugh-Maps","Multiplexers","Decoders","Prpogation-and-Contamination-Delay"],"tags":[],"content":"main resource: www.amazon.com/Digital-Design-Computer-Architecture-ARM/dp/0128000562\nChapter 1\n\nAnalog circuits - Used to make amplifiers; can input and output a continuous range of voltages\nDigital  circuits - restrict voltages to discrete ranges, which we will use to indicate 0 and 1.\nMicroarchitecture - links the logic and architecture levels of abstraction.\narchitecture - abstraction describe a computer from the programmaer‚Äôs perspective.\nThe digital abstraction\nThe amoutn of information D in a discrete valued variable with N distinct states is measured in units of bits as\nD=log2‚ÄãNbits\nBit is short for binary digit.\nContinuuous singla theoretical contain infinite amount of infomraiton. Inm pracitce, measurement error limit the information to only 10 to 16 bits for mosot cononitnuoos singals.\nGeorge Boole developed system of logic operating on systems known as boolean logic.\nBytes, nibbles, and all that jazz\nA group of eight bits is called a byte. It represents one of 2 8 = 256 possibilities. The size of objects stored in computer memories is customarily measured in bytes rather than bits.\nA group of four bits, or half a byte, is called a nibble. It represents one of 2 4 = 16 possibilities. One hexadecimal digit stores one nibble and two hexadecimal digits store one full byte. Nibbles are no longer a commonly used unit, but the term is cute.\nMicroprocessors handle data in chunks called words. The size of a word depends on the architecture of the microprocessor. When this chapter was written in 2015, most computers had 64-bit processors, indicating that they operate on 64-bit words. At the time, older computers handling 32-bit words were also widely available. Simpler microprocessors, especially those used in gadgets such as toasters, use 8- or 16-bit words.\nMicroprocessor is a processor built on a single chip.\nLogic gates\nBuffer - From the logical point of view, a buffer is no different from a wire, so it might seem useless. However, from the analog point of view, the buffer might have desirable characteristics such as the ability to deliver large amounts of current to a motor or the ability to quickly send its output to many gates. This is an example of why we need to consider multiple levels of abstraction to fully understand a system; the digital abstraction hides the real purpose of a buffer.\nBeneath the digital abstraction\nVOL‚Äã ‚áí output low\nVIL‚Äã ‚áí input low\nVOH‚Äã ‚áí output high\nVIH‚Äã ‚áí input high\nVDD‚Äã ‚áí highest voltage in the system croms from the power supply and is usually called V_DD\nIn 1970‚Äôs and 1980‚Äôs technology, V DD was generally 5 V. As chips have progressed to smaller transistors, V DD has dropped to 3.3 V, 2.5 V, 1.8 V, 1.5 V, 1.2 V, or even lower to save power and avoid overloading the transistors.\nSupply voltage\nLogic Levels\nNoise margins\n\nVDD‚Äã stands for the voltage on the drain of a metal-oxidesemiconductor transistor, used to build most modern chips. The power supply voltage is also sometimes called VCC‚Äã , standing for the voltage on the collector of a bipolar junction transistor used to build chips in an older technology. Ground is sometimes called VSS‚Äã because it is the voltage on the source of a metal-oxidesemiconductor transistor.\nDC Transfer Characteristics\nDC indicates behavior when an input voltage is held constant or changes slowly enough for the rest of the system to keep up. The term‚Äôs historical root comes from direct current, a method of transmitting power across a line with a constant voltage. In contrast, the transient response of a circuit is the behavior when an input voltage changes rapidly.\nAn ideal inverter\n\na ‚áí ideal inverter\nb ‚áí more realistic inverter\nA reasonable place to choose the logic levels is where the slope of the transfer characteristic dV(Y) / dV(A) is ‚àí1. These two points are called the unity gain points. Choosing logic levels at the unity gain points usually maximizes the noise margins. If V IL were reduced, V OH would only increase by a small amount. But if V IL were increased, V OH would drop precipitously.\nStatic discipline\nTo avoid inputs falling into the forbidden zone, digital logic gates are designed to conform to the static discipline. The static discipline requires that, given logically valid inputs, every circuit element will produce logically valid outputs.\nFour major logic families that predominated from the 1970‚Äôs through the 1990‚Äôs are Transistor-Transistor Logic (TTL), Complementary MetalOxide-Semiconductor Logic (CMOS, pronounced sea-moss), Low Voltage TTL Logic (LVTTL), and Low Voltage CMOS Logic (LVCMOS).\n\nRobert Noyce, 1927‚Äì1990. Born in Burlington, Iowa. Received a B. A. in physics from Grinnell College and a Ph.D. in physics from MIT. Nicknamed ‚ÄúMayor of Silicon Valley‚Äù for his profound influence on the industry.\nCofounded Fairchild Semiconductor in 1957 and Intel in 1968. Coinvented the integrated circuit. Many engineers from his teams went on to found other seminal semiconductor companies\nCMOS Transistors\nTransistors: electrically controlled switches that turn ON or OFF when a voltage or current is applied to a control terminal.\nTwo main types:\n\nbipolar junction transistors\nmeta-oxide semiconductor field effect transistors (MOSFETs or MOS transistors)\n\nSemiconductors\n\n\nMade from silicon  (group IV atom)\n\nby itself is a bad conduct\nWhen you add small amounts of impurities, dopant atoms it becomes a better conductor\n\n\n\n\nThe hole is a lack of nega- tive charge, so it acts like a positively charged particle. Hence, we call boron a p-type dopant. Because the conductivity of silicon changes over many orders of magnitude depending on the concentration of dopants, sili- con is called a semiconductor.\nDiodes\nThe junction between p-type and n-type silicon is called a diode. The p-type region is called the anode and the n-type region is called the cathode\n\nPower Consumption\nCombinational Logic Design\nBubble Pushing\nKarnaugh Maps\nCombinational Building blocks\nMultiplexers\nDecoders\nTiming\nPrpogation and Contamination Delay\nSequential Logic Design\nHardware Description Languages"},"Computer-Architecture/CMOS":{"title":"CMOS","links":["MOSFET","Computer-Architecture/PMOS","Computer-Architecture/NMOS","Computer-Architecture/inverter"],"tags":[],"content":"www.youtube.com/watch\nComplimentary Metal oxide semiconductor\n\nused to perform logical functions\n\nConsists of two types of MOSFET transistors\n\n\nPMOS (from p-channel metal-oxide-semiconductor) - missing electron\n\n\nNMOS (electron fill)\nCMOS is essentially an inverter\n\n\n"},"Computer-Architecture/Compute-in-Memory-(CIM)":{"title":"Compute-in-Memory (CIM)","links":[],"tags":[],"content":"Address two critical bottlenecks in conventional computing architectures:\n\nmemory latency - time it takes to move data between the memory and the process\nenergy consumption - spike due to frequent data transfers between memory and CPU\n"},"Computer-Architecture/Computer-Architecture":{"title":"Computer Architecture","links":["Computer-Architecture/ARM-architecture"],"tags":[],"content":"ARM architecture"},"Computer-Architecture/NMOS":{"title":"NMOS","links":[],"tags":[],"content":"electron concentrated\n\np-type substrate"},"Computer-Architecture/PMOS":{"title":"PMOS","links":[],"tags":[],"content":"\nn-type substrate"},"Computer-Architecture/Parallel-Processors":{"title":"Parallel Processors","links":["Graphic-Double-Data-Rate-(GDDR)-DRAM"],"tags":[],"content":"\nGraphic Double Data Rate (GDDR) DRAM\n\n"},"Computer-Architecture/index":{"title":"Computer Architecture","links":[],"tags":[],"content":""},"Computer-Architecture/inverter":{"title":"inverter","links":["Computer-Architecture/AC"],"tags":[],"content":"source: www.youtube.com/watch\nConverts a DC source to a AC source\nDC ex:\n\nphotovoltaic cell\nbattery\n\nAC load ex:\n\nAC motor\nsynchronous motor\n\nThe idea is to essentially flip the direction of the positive and negative voltage every so often for the AC load (?)\n"},"Computer-Vision/Slam":{"title":"Slam","links":[],"tags":[],"content":"From the eInfoChips talk at Embedded Vision Summitcat test.md\nGPU-Accelerated RTAB-Map SLAM by eInfochips\nüìå Overview\neInfochips, a subsidiary of Arrow Electronics, presented their advancements in enhancing RTAB-Map‚Äôs performance through GPU acceleration at the Embedded Vision Summit 2025. Their focus was on optimizing LiDAR-based SLAM for real-time applications on NVIDIA platforms. Bluesky Social+2X (formerly Twitter)+2X (formerly Twitter)+2\n\nüß≠ Understanding SLAM and RTAB-Map\nWhat is SLAM?\nSimultaneous Localization and Mapping (SLAM) enables a robot to build a map of an unknown environment while simultaneously tracking its position within it. Key challenges include:\n\nSensor Drift: Over time, sensors like IMUs can drift, leading to inaccuracies.\nEnvironmental Changes: Dynamic environments can cause mapping failures.\nKidnapped Robot Problem: If a robot is moved without tracking (e.g., lifted and placed elsewhere), it may lose its position estimate.\n\nRTAB-Map Features\nRTAB-Map (Real-Time Application-Based Mapping) is a graph-based SLAM approach supporting various sensors, including RGB-D cameras, stereo cameras, and LiDAR. Notable features:\n\n\nMulti-Sensor Support: Integrates data from different sensor types.\n\n\nRobust Localization: Offers improved localization compared to some other SLAM methods.\n\n\nMulti-Session Mapping: Capable of building maps over multiple sessions.\n\n\nROS Compatibility: Easily integrates with ROS\n\n\n\n‚öôÔ∏è GPU Acceleration in RTAB-Map\nTo enhance real-time performance, eInfochips implemented GPU acceleration in RTAB-Map, focusing on both the frontend and backend processes.\nFrontend Enhancements\n\nShort-Term Memory (STM): Processes incoming point cloud frames to create a local map.\nPoint Cloud Processing: Utilizes GPU acceleration for tasks like filtering and ground segmentation.\nSynchronization: Ensures synchronized data processing across sensors.\n\nBackend Enhancements\n\n\nMap Optimization: Accelerates graph optimization processes using GPU capabilities.\n\n\nLoop Closure Detection: Enhances the detection of previously visited locations for map correction.\n\n\n\nüß™ Experimental Setup and Results\n\n\nPlatform: Implemented on a ROS2-based RTAB-Map pipeline.\n\n\nInput Data: Processed point clouds of size 512x512 at 18 FPS.\n\n\nPerformance Metrics:\n\n\nTracking Error: Assessed the accuracy of the robot‚Äôs estimated trajectory.\n\n\nMap Quality: Evaluated the fidelity of the generated maps.\n\n\nFPS Maintenance: Achieved higher frame rates without compromising map quality.\nThis was done on an AGX and AGX Orin with Jetpack 5.1.2 for CUDA version purposes (not sure - don‚Äôt think they knew what they were doing here)\n\n\n\n\n\nüìä Analysis: KD-Tree vs. Octree\n\n\nKD-Tree: Efficient for organizing point clouds but computationally intensive.\n\n\nOctree: Offers a more GPU-friendly structure, enabling faster processing with comparable accuracy.\n\n\nBy implementing octrees on GPUs, eInfochips leveraged the parallel processing capabilities to accelerate point cloud operations such as filtering and ground segmentation. This approach aligns with findings from other studies, which have demonstrated significant performance improvements using GPU-based octree implementations for tasks like ray shooting in volumetric mapping.\n\nüìà Efficiency Metrics\nTo measure SLAM efficiency:\n\n\nTracking Error: Difference between estimated and actual positions.\n\n\nMap Quality: Accuracy and completeness of the generated map.\n\n\nFrame Rate (FPS): Higher FPS indicates better real-time performance.\n\n\nLocalization Accuracy: Precision of the robot‚Äôs position estimation.\n\n"},"Cryptography/Cryptography":{"title":"Cryptography","links":["resources/Introduction_to_Modern_Cryptography.pdf","private-key","Cryptography/Mono-alphabetic-substition","Cryptography/The-Vigen√®re-(poly-alphabetic-shift)-cipher"],"tags":[],"content":"main resource Introduction_to_Modern_Cryptography.pdf\nChapter 1: Introduction and classical ciphers\nSetting of Private-key encryption\n\nsetting in which communicating parties share some secret information in advance is private-key (or symmetric-key) setting\n\nThe syntax of encryption\nPrivate-key encryption scheme\n\nkey-generation algorithm: Gen is a probabilistic algorithm that outputs a key k chosen according to some distribution\nencryption algorithm: Enc take as a input a key k and a plantext m and outputs a ciphertext c  ‚Üí Enck‚Äã(m)\ndecryption algorithm: Dec takes input a key k and ciphertext c and outputs a plaintext m ‚Üí Deck‚Äã(c)\n\nGenerating keys defines a key space K  (i.e. set of all possible keys), and encryption scheme is defined over some set of possible plaintext messages denoted M\nBasic correct requirement of any encryption scheme is that for every key k by Gen and every plaintext messagr m‚ààM, it holds that\nDeck‚Äã(Enck‚Äã(m))=m\nIn words, an encryption scheme must have the property that decrypting a ciphertext (with the appropriate key) yields the original message that was encrypted.\nKeys and Kerckoff‚Äôs principle\nIf adversary knows the algorithm Dec as well as the key k shared by two communicating parties, then that adversary will be able to decrypt all communication between these parties.\nKerckhoffs‚Äô principle: The cipher method must not be required to be secret, and it must be able to fall into the hands of the enemy without inconvenience.\nHistorical Ciphers and their crpytoanalysis\n\nCaesar‚Äôs cipher\nMono-alphabetic substition\nThe Vigen√®re (poly-alphabetic shift) cipher\n"},"Cryptography/Mono-alphabetic-substition":{"title":"Mono-alphabetic substition","links":[],"tags":[],"content":"\nAssociate letters of the english alphabet with numbers 0,...,25. Let pi‚Äã, for 0‚â§i‚â§25 denote probability of ith letter in normal English text\nSimple calculation gives ‚àëi=025‚Äãpi2‚Äã‚âà0.065\nSay we are given some ciphertext and let qi‚Äã denote the probability of the ith letter. (qi‚Äã is simple the number of occurences of the ith letter divided by the length of the ciphertext). If the key is k then we can expect that qi+k‚Äã should be roughly equal to pi‚Äã for every i [They are using i+k instead of the more cumbersome [i+k mod 26]]. Equivalent if we compute\n"},"Cryptography/STARKs":{"title":"STARKs","links":[],"tags":[],"content":""},"Cryptography/The-Vigen√®re-(poly-alphabetic-shift)-cipher":{"title":"The Vigen√®re (poly-alphabetic shift) cipher","links":[],"tags":[],"content":"\nmapping different instances of the same plaintext character to different ciphertext characters\nthis has the effect of ‚Äúsmoothing out‚Äù the probability distribution of characters in the ciphertext\n"},"Cryptography/index":{"title":"Cryptography","links":[],"tags":[],"content":""},"Data-Structures-and-Algos/KD-Tree":{"title":"KD Tree","links":[],"tags":[],"content":"Allows to quickly find approximtae nearest neibors in a low-dimensional real-valued space"},"Data-Structures-and-Algos/index":{"title":"index","links":["Data-Structures-and-Algos/KD-Tree"],"tags":[],"content":"\nKD Tree\n\n"},"Economics/Wealth-of-Nations":{"title":"Wealth of Nations","links":[],"tags":[],"content":"Plan of the book:\nMain ideas:\n\nHow the annual labor of a nation produces the necessary and convenient goods it consumes\nWealth of a nation\n"},"Economics/index":{"title":"index","links":["Economics/Wealth-of-Nations"],"tags":[],"content":"\nWealth of Nations\n"},"Education/index":{"title":"index","links":[],"tags":[],"content":"An attempt to understand, improve, and henceforth revolutionize educations"},"Large-Model-Systems/1-bit-LLMs":{"title":"1 bit LLMs","links":["resources/1-bit-LLMs.pdf","BitNet-b1.58"],"tags":[],"content":"resource: 1-bit-LLMs.pdf\nBitNet b1.58\n\nevery single parameter (or weight) of the LLM is ternary {-1, 0, 1}\n\n"},"Large-Model-Systems/Allreduce":{"title":"Allreduce","links":[],"tags":[],"content":"Each of N workers is responsible for:\n\nsumming 1/N gradients collected from (N-1) peers\nDistributing the sums of the (N-1) peers\n\n‚ÄùRing‚Äù red"},"Large-Model-Systems/CLIP":{"title":"CLIP","links":[],"tags":[],"content":"\nContrastive Language-Image Pre-training\n"},"Large-Model-Systems/CUDA-API-of-GPT-2":{"title":"CUDA API of GPT 2","links":["Large-Model-Systems/Token-embeddings-matrix","Large-Model-Systems/Positional-embeddings-matrix"],"tags":[],"content":"Cuda basics\n\nThread smallest unit of execution\nBlock: group of threads that can share memory\nGrid group of blocks that executes a kernel across a dataset\n\nMemory hierarchy\n\nRegisters: fastest memory, private to each thread\nShared memory: shared among threads within the same block, faster than global memory but limited in size\nGlobal memory: accessible by all threads but slower than registers or shrared memory\n\nCore reference: github.com/viraatdas/llm.c/blob/master/train_gpt2.c\n\nHeader inclusion and Definitions\n\n#include &lt;stdio.h&gt;\n#include &lt;stdlib.h&gt;\n#include &lt;ctype.h&gt;\n#include &lt;stdint.h&gt;\n#include &lt;assert.h&gt;\n#include &lt;math.h&gt;\n#include &lt;time.h&gt;\n#include &lt;string.h&gt;\n#include &lt;unistd.h&gt;\n#ifdef OMP\n#include &lt;omp.h&gt;\n#endif\n\nCore Functions (Forward and Backward Passes)\n\nencoder_forward Function\n__global__ void encoder_forward_kernel2(float* out,\n                               int* inp, float* wte, float* wpe,\n                               int B, int T, int C) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    int N = B * T * C;\n \n    if (idx &lt; N) {\n        int bt = idx / C;\n        int b = bt / T;\n        int t = bt % T;\n        int c = idx % C;\n \n        int ix = inp[b * T + t];\n \n        float* out_btc = out + b * T * C + t * C + c;\n        float* wte_ix = wte + ix * C + c;\n        float* wpe_tc = wpe + t * C + c;\n        *out_btc = *wte_ix + *wpe_tc;\n    }\n}\n \nParameters:\n\nout: Output array where the embeddings will be stored.\ninp: Input array containing token indices.\nwte: Token embeddings matrix\n\neach row corresponds to a unique token (word or subword) in the models vocabulary\n\n\nwpe: Positional embeddings matrix\n\ncaptures the positional information of tokens within the input data\n\n\nB, T, C: Dimensions representing Batch size, Sequence length, and Number of Channels (embedding dimension), respectively.\n\nBreakdown\n\nKernel declaration: __global__ indicates it‚Äôs a CUDA kernel called from the host (CPU) and executed on the  device (GPU)\nThread index calculation:\n\nidx = blockIdx.x * blockDim.x + threadIdx.x;: Calculates the global index of the thread across blocks.\n\n\nDimension calculations:\n\nN = B * T * C;: Total number of elements to process.\n\n\nCore computation\n\nint ix = inp[b * T + t];: Fetches the index from the input array to select which embeddings to use.\nfloat* out_btc = out + b * T * C + t * C + c;: Computes the pointer to the output location.\nfloat* wte_ix = wte + ix * C + c;: Computes the pointer to the selected token embedding.\nfloat* wpe_tc = wpe + t * C + c;: Computes the pointer to the position embedding.\n*out_btc = *wte_ix + *wpe_tc;: Performs the addition of token and position embeddings and stores the result in the output array.\n\n\n"},"Large-Model-Systems/Deep-Dive-of-GPT-architecture":{"title":"Deep Dive of GPT architecture","links":["Large-Model-Systems/Tokenization","Sequence-Creation","Large-Model-Systems/Transformers","Embedding-Layer","Transformer-Blocks","Normalization-and-Residual-Connections"],"tags":[],"content":"1. Data Structuring\nPreprocessing\n\nCleaning\nTokenization:\nSequence Creation: Sequences of fixed length (eg. 512 tokens) are created, and special tokens like [START] and [END] may be added to denote the beginning and end of sequences.`\n\n2. Model Architecture\nCore components\n\nTransformers\n\nBlock Structure\n\nEmbedding Layer:\nTransformer Blocks\n\nMulit-head Self-attention Layer\nFeedforward Neural Network\n\n\nNormalization and Residual Connections\n\nOutput Layer\n\noutput of the last transformer block is passed to a final linear layer, which projects the hidden stat eback to the size of the vocabulary. This is followed by a softmax layer to generate probabilities of the next token.\n\n3. Training\nGPT is trained to predict the next token in the sequence given the previous tokens, maximizing the likelihood of the next token based on the preciding context.\nLoss function: Cross-entropy loss is used, where the model learns to reduce the discrepancy between predicted probability distribution and the actual distrbution token in the training data"},"Large-Model-Systems/Gemini":{"title":"Gemini","links":["resources/Gemini.pdf","Flamingo","Coca","PaLI","Video-understanding","Universal-Speech-Model-(USM)","TPUv5e","Large-Model-Systems/TPUv4","4096-chips","optical-switch","single-controller-programming-model-of-Jax","Pathways","GSPMD-practioner","XLA","MegaScale-XLA","Large-Model-Systems/Silent-Data-Corruption-(SDC)","SentencePiece-tokenizer","Hoffmann-et-al.-(2022)","Touvron-et-al.-(2023a)"],"tags":[],"content":"resource: Gemini.pdf\nModel Architecture\n\n\nbuild on top of Transformer decoders\n\n\nVisual encoding was done based on the work of Flamingo, Coca, PaLI\n\n\nVideo understanding is accomplished by encoding the video as a sequence of frames in the large context window.\n\nThey can be interleaved naturally with text or audio as part of the model input\n\n\n\nCan also ingest audio signals at 16kHz from Universal Speech Model (USM)\n\n\nTraining Infrastructure\n\ntrained on TPUv5e and TPUv4\n\nTPUv4:\nTPUv4 accelerators are deployed in ‚ÄúSuperPPods‚Äù of 4096 chips, each connected to a dedicated optical switch, which can dynamically reconfogire 4x4x4 chip cubes into arbitrary 3d torus topologies in around 10 seconds.\nTPU accelerators communicate over high spped inter-chip-interconnect, but at Gemini ultra scale, we combine SuperPods in multiple datacenters using Google‚Äôs intra-cluster and inter-cluster network.\nThe single controller programming model of Jax and Pathways allows a single Python process to orchestrate the entire training run, dramatically simplifying the develoment workflow.\nThe GSPMD practioner in the XLA compiler partitions the training step computation, and the MegaScale XLA compiler pass statically schedules\nMaintaining goodput\ngoodput: time spent computing useful new steps over the elapsed time of the training jobs\nThey didn‚Äôt use conventional approach of periodic checkpointing of weights to persistent cluster storage. Why?\nThe conventional approach of periodic checkpointing of weights to persistent cluster storage becomes highly impractical at the scale at which Gemini models operate for several reasons:\n\n\nScale of the Model and Data: Gemini models are trained on a very large scale, using a large fleet of TPU accelerators across multiple data centers. The sheer volume of data (model weights and states) that would need to be periodically saved is massive. Periodically writing this vast amount of data to storage would require substantial bandwidth and would significantly slow down the training process.\n\n\nFrequency of Hardware Failures: At such a large scale, hardware failures become commonplace, not just a rare occurrence. Periodic checkpointing assumes a relatively stable hardware environment where failures are exceptions rather than the norm. Given the high rate of unplanned hardware failures, relying solely on periodic checkpointing would lead to frequent interruptions and potential data loss, making it a risky and inefficient approach for ensuring model state preservation.\n\n\nRecovery Time: In the event of a failure, recovering the model state from persistent storage can be time-consuming, especially when dealing with the vast datasets and model sizes in question. This recovery process would significantly reduce the overall goodput of the training job, as the system would spend a non-trivial amount of time simply reloading data from storage, rather than performing useful computations.\n\n\nTo address these challenges, Gemini employs a strategy of making use of redundant in-memory copies of the model state. In the event of an unplanned hardware failure, the system can rapidly recover directly from an intact model replica, which is significantly faster than reloading from persistent storage. This approach allows for a substantial speedup in recovery time and minimizes the impact of hardware failures on the overall training process, thereby increasing the goodput from 85% to 97%‚Äã\nFor gemini, they instead made use of redundant in memory-copies of the model state and on any unplanned hardware failure, they rapidly recover directly from an intact model replica. This provides a substantial speedup in recovery, despite the significantly larger training resources.\nInteresting system failure modes\n\nSilent Data Corruption (SDC)\n\nTraining Dataset\n\ntrained on both multimodal and multilingual data\npretraining uses data from web documents, books, and codde and includes image, audio, and video\n\nDetails:\n\nUsed SentencePiece tokenizer\nThe number of tokens used to train the largest models were determined following the approach in Hoffmann et al. (2022). The smaller models are trained for significantly more tokens to improve performance for a given inference budget, similar to the approach advocated in Touvron et al. (2023a)\n\nEvaluation\nImage generation\nGemini is able to output images natively, without having to rely on an intermediate natural language description that can bottleneck the model‚Äôs ability to express images. This uniquely enables the model to generate images with prompts using interleaved sequences of image and text in a few-shot setting.\nWe use multi-objective optimization with a weighted sum of reward scores from helpfulness, factuality, and safety, to train a multi-headed reward model."},"Large-Model-Systems/How-a-Transformer-works-at-inference-vs-training-time":{"title":"How a Transformer works at inference vs training time","links":[],"tags":[],"content":"Source: www.youtube.com/watch\nnotes generated using recall\nHow a Transformer works at inference vs training time\n\nSource URL\nTransformer Architecture\n\nA Transformer consists of an encoder and a decoder.\nThe encoder converts the input IDs into a sequence of hidden states (embedding vectors).\nThe decoder generates text one token at a time conditioned on the encoder‚Äôs final hidden states and the decoder input IDs.\nThe language modeling head maps the final hidden state of the Transformer decoder to a vector of logits for each token in the vocabulary.\n\nInference\n\nAt inference time, the Transformer doesn‚Äôt generate the entire translation in one go.\nThe source sentence is tokenized and converted into input IDs.\nThe model generates text one token at a time by selecting the token with the highest logit score at each time step.\nThe generation process continues until the decoder predicts the end-of-sequence token.\n\nTraining\n\nAt training time, the source and target sentences are tokenized and converted into input IDs.\nThe encoder generates final hidden states for the source sentence, which are fed to the decoder along with the decoder input IDs.\nThe decoder outputs a sequence of hidden states, which are then passed through a language modeling head to compute logits for each token.\nThe cross-entropy loss between the ground truth labels and the model‚Äôs predictions is computed, and the loss is backpropagated to update the model‚Äôs weights.\nDuring training, multiple sequences are typically batched together, and the input IDs and labels are padded to ensure a fixed sequence length.\nThe model is trained using a single forward pass with teacher forcing.\n"},"Large-Model-Systems/IEEE-754":{"title":"IEEE-754","links":[],"tags":[],"content":""},"Large-Model-Systems/LLaVA":{"title":"LLaVA","links":["resources/LLaVA.pdf","Large-Model-Systems/CLIP","Vicuna"],"tags":[],"content":"\nMultimodal language-image instruction\nresource: LLaVA.pdf\n\nData Generation\n\nGPT-4: Used for generating instruction-following data\nCLIP (Vision Encoder): This is for [Visual instruction tuning]\nVicuna (Language Model)\n\nGPT-assisted Visual Instruction Data Generation\nVisual instruction tuning\n"},"Large-Model-Systems/Liger-Kernels":{"title":"Liger Kernels","links":[],"tags":[],"content":""},"Large-Model-Systems/LlaMa-from-scratch":{"title":"LlaMa from scratch","links":[],"tags":[],"content":"source: www.youtube.com/watch"},"Large-Model-Systems/ML-Interpretability":{"title":"ML Interpretability","links":[],"tags":[],"content":"www.youtube.com/watch"},"Large-Model-Systems/MatMul-Free-Language-Modeling":{"title":"MatMul-Free Language Modeling","links":["Large-Model-Systems/Vector-matrix-multiplication-(VMM)"],"tags":[],"content":"source: arxiv.org/pdf/2406.02528\n\nVector-matrix multiplication (VMM)\n\n"},"Large-Model-Systems/Optical-Circuit-Switches":{"title":"Optical Circuit Switches","links":[],"tags":[],"content":"An Optical Circuit Switch (OCS) is a network device that uses optical technology to switch data paths between optical fibers. Unlike traditional electronic switches, which convert optical signals to electrical signals for switching and then back to optical signals, OCSes handle the data entirely in the optical domain. This approach provides several benefits, including reduced latency, higher bandwidth, and lower power consumption."},"Large-Model-Systems/Positional-embeddings-matrix":{"title":"Positional embeddings matrix","links":[],"tags":[],"content":"What\nPositional embeddings are vectors added to token embeddings to encode position of each token in the sequence or positional information of tokens within the input data.\nPositional Embeddings?\n\nVectors added to token embeddings to encode the position of each token in the sequence.\n\nStructure of the positional embedding matrix\nGiven:\n\nB: Batch size\nT: Sequence length\nC: Embedding size\n\nThe positional embedding matrix wpe has a shape of [T, C]. Each row corresponds to a positional embedding vector for a respective position in the sequence, and each column represents a dimension of the embedding space.\nExample:\nImagine a simplified scenario where:\n\nT = 4 (sequence length)\nC = 3 (embedding dimensions)\n\nThe positional embedding matrix might look like this:\nwpe = [\n    [0.1, 0.2, 0.3],  # Positional embedding for position 0\n    [0.4, 0.5, 0.6],  # Positional embedding for position 1\n    [0.7, 0.8, 0.9],  # Positional embedding for position 2\n    [1.0, 1.1, 1.2]   # Positional embedding for position 3\n]\n\nUnderstanding the role of numbers in the wpe matrix\nEach element in the wpe matrix represents a feature of the position‚Äôs embedding. Let‚Äôs break down their meaning and function:\nHow do these values affect processing?\nLearning During Training: Initially, if the embeddings are randomly initialized, these values might not have a meaningful pattern. During training, through backpropagation, these values are adjusted to minimize prediction error, gradually learning to encode useful positional signals.\nGradient Updates: As the model is exposed to more data during training, the gradients (derived during backpropagation) update these values such that the positional information they encode helps the model predict the next word in a sentence more accurately or generate more coherent text."},"Large-Model-Systems/Post-Training-Static-quantization":{"title":"Post-Training Static quantization","links":[],"tags":[],"content":""},"Large-Model-Systems/Quantization":{"title":"Quantization","links":["Large-Model-Systems/Post-Training-Static-quantization","Dynamic-Quantization","BigNum-arithmetic","Large-Model-Systems/IEEE-754","Multiply-Accumulate-(MAC)"],"tags":[],"content":"Converting a model from using FP (like 32-bit floats) to using lower-precision formats (like 8-bit integers).\nHow does quantization work?\n\nPost-Training Static quantization: Entire model including weights and activations are converted to lower precision. Model is calibrated using a small calibration dataset to minimize the impact on accuracy. May not always have best accuracy.\nDynamic Quantization: Weights quantized statically, but activations are quantized dynamocally at runtime. Method is often used for models wehre activation ranges can vary significantly depending on the input data.\nQuantization-Aware Training (QAT)\n\nsource: www.youtube.com/watch\nProblem\n\nSmallest Llama 2 has 7 billion parameters. If every parameter is 32 bit, then we need 8‚àó1097‚àó109‚àó32‚Äã=28GB just to store parameters on disk\nFor inference, we need to load all its parameters in memory\n\nHow are integers represented in the CPU (or GPU)?\n\nPython can represent arbitraty big numbers by using so BigNum arithmetic: each number is stored as an array of digits in base 230\n\nHow are floating point numbers represented?\nIEEE-754\nApplying quantization\n\n\nQuantization first happens\nThen the integ arithmetic happens\nand then it‚Äôs deuqnatized and sent to the next layer\n\nTypes of qunatization\nAssymetric vs Symmetric quantization\n\n\nLow-precision matrix multiplication\n\nMultiply-Accumulate (MAC)\nHow to choose range\nMin-max:\n\nŒ±=max(V)\nŒ≤=max(V)\nSensitive to outliers\n\nPercentile"},"Large-Model-Systems/SSM-vs-Transformer":{"title":"SSM vs Transformer","links":[],"tags":[],"content":"Brainchip talk from the Embedded Vision Summit\n## SSM Overview\n\n## Training \nTraing SSM as convolutional netwroks explotiing parallelism on GPU \n\n## SSM are Markov\n"},"Large-Model-Systems/Silent-Data-Corruption-(SDC)":{"title":"Silent Data Corruption (SDC)","links":[],"tags":[],"content":"\nIt‚Äôs rare, but in the context of Gemini means that we can expect SDC vents to impact training every week or two.\nRapidly detecting and removing faulty hardware required several new techniques that exploit deterministic replay t isolate incorrect computations\nproactive SDC scanner on idle machines and hot standbys\n\nThis could be incorrect computation issue that the software doesn‚Äôt check for"},"Large-Model-Systems/Softmax":{"title":"Softmax","links":[],"tags":[],"content":"softmax(zi‚Äã)=‚àëj‚Äãezj‚Äãezi‚Äã‚Äã\nz is vector containing the raw class scores from the last layer of hte network, ezi‚Äã is the exponential of the score for class i, and the deonminator is the sum of exponentials of all raw class scores, which acts as a normalization constanct."},"Large-Model-Systems/SparseCore":{"title":"SparseCore","links":[],"tags":[],"content":"\nDedicated architecture for accelerating embeddings in Deep Learning Recommendation Models (DLRMs).\nSupports efficient dataflow and memory operations, improving performance for sparse operations.\nContributes to the system‚Äôs overall performance without significant area and power costs.\n"},"Large-Model-Systems/TPUv4":{"title":"TPUv4","links":["Large-Model-Systems/Optical-Circuit-Switches","Large-Model-Systems/SparseCore"],"tags":[],"content":"source: TPU v4: An Optically Reconfigurable Supercomputer for Machine Learning with Hardware Support for Embeddings\nOverview\n\nTPU v4 is designed to handle the evolving scale and complexity of machine learning models.\nUses Optical Circuit Switches (OCSes) for dynamic reconfiguration of interconnect topology, improving various aspects such as scale, availability, utilization, and performance.\nIncludes SparseCores to accelerate models relying on embeddings, achieving 5x‚Äì7x speed improvements while using only 5% of the die area and power.\n\nKey Features\n\nOptical Circuit Switches (OCSes)\nSparseCore (SC)\n"},"Large-Model-Systems/Token-embeddings-matrix":{"title":"Token embeddings matrix","links":[],"tags":[],"content":"Definition\n\nToken Embedding Matrix (wte): This matrix is a learned representation where each row corresponds to a unique token (word or subword) in the model‚Äôs vocabulary. The columns represent the features or dimensions of the embedding space. This matrix is typically learned during the training process of the model.\n\nStructure and Dimensions\n\nDimensions: If the vocabulary size of the model is VV and the embedding dimension is CC, then the token embedding matrix will have the dimensions V√óCV√óC. For instance, if there are 50,000 tokens in the vocabulary and each embedding is 768-dimensional, the matrix size will be 50,000 x 768.\nContent: Each row in this matrix is a vector representing a token. The vector is not human-readable and generally doesn‚Äôt have an interpretable meaning in conventional terms. It‚Äôs a high-dimensional representation that the model learns to associate with the semantics and usage patterns of the token.\n\nHow It Works\n\nLookup Process: During the forward pass, the model receives a sequence of token indices as input. These indices are used to ‚Äúlook up‚Äù or retrieve the corresponding embedding vectors from the token embedding matrix. This process is often referred to as an embedding lookup.\nExample: Suppose the word ‚Äúhello‚Äù is represented by index 423 in the vocabulary. The model will access the 423rd row of the token embedding matrix to fetch the embedding vector for ‚Äúhello‚Äù.\n\nImportance\n\nSemantic Representation: The embeddings capture semantic and syntactic aspects of the tokens. Tokens that are used in similar contexts tend to have closer embeddings in the vector space, a property useful for many language tasks.\nTraining: Initially, these embeddings are random but learn to capture meaningful representations through the backpropagation of error gradients during training.\n\nVisualization (Conceptual)\nHere‚Äôs a simplified conceptual view of what a part of a token embedding matrix might look like:\n+---------+------------------+------------------+-----+------------------+\n| TokenID | Feature_1        | Feature_2        | ... | Feature_C        |\n+---------+------------------+------------------+-----+------------------+\n| 1       | 0.786            | -1.234           | ... | 0.045            |\n| 2       | -0.560           | 0.990            | ... | -0.112           |\n| ...     | ...              | ...              | ... | ...              |\n| 423     | 0.158            | 0.732            | ... | -0.903           |\n| ...     | ...              | ...              | ... | ...              |\n| V       | -0.369           | 0.209            | ... | 1.334            |\n+---------+------------------+------------------+-----+------------------+\n\n\nIn this table, each row under ‚ÄúFeature_1‚Äù to ‚ÄúFeature_C‚Äù is the embedding of the token identified by ‚ÄúTokenID‚Äù.\nUsage\nThe embeddings are used as the first transformation step in models to convert categorical token information into a form that the subsequent layers (like attention layers in GPT-2) can process effectively.\nWould you like to delve into how these embeddings are used in training, or perhaps explore more about positional embeddings or other components of the model?"},"Large-Model-Systems/Tokenization":{"title":"Tokenization","links":[],"tags":[],"content":"From the Image Tokenization talk from Google and Verisilicon for Edge Vision Summit\nUsing VAE for tokenization (?????)\nTokenizer is a Feature Extractor\nTokenization Creates a form of data compression\nNeural Cascade\nwhy isn‚Äôt there a bfloat8"},"Large-Model-Systems/Torchserve":{"title":"Torchserve","links":["HTTP-2.0","Data-Parallel","Large-Model-Systems/Allreduce"],"tags":[],"content":"resource: pytorch.org/serve/large_model_inference.html\nHow it works\n\nTorchServe sets distributed environment\nUses round-robin to assign GPUs to a worker on a host\n\nin case of large inference, we can specify based on model_config.yaml\n\n\nCUDA_VISIBLE_DEVICES is based on this number\n\nex:\nFor instance, suppose there are eight GPUs on a node and one worker needs 4 GPUs (ie, nproc-per-node=4) on a node. In this case, TorchServe would assign CUDA_VISIBLE_DEVICES=‚Äù0,1,2,3‚Äù to worker1 and CUDA_VISIBLE_DEVICES=‚Äù4,5,6,7‚Äù to worker2.\n\nYou can use Pippy integration\nPippy integration: Pipeline Parallelism for PyTorch\n\nwww.deepspeed.ai/tutorials/pipeline/\nIt partitions the layers of a model into stages that can be processed in parallel.\n\nHow Pipeline Parallelism Works\n\n\nModel Segmentation: The entire neural network model is divided into several segments. Each segment can be seen as a subset of consecutive layers of the model.\n\n\nDevice Allocation: Each segment of the model is assigned to a different device. This way, multiple devices can be used to host the entire model, with each device responsible for computing the forward and backward passes of its assigned segment.\n\n\nMini-batch Splitting: The input data (mini-batch) is also split into smaller micro-batches. These micro-batches are then fed sequentially into the pipeline.\n\n\nSequential Processing of Micro-batches: The first micro-batch is fed into the first segment. Once the first device starts processing the first micro-batch, it can immediately pass its output to the second segment/device, and then start processing the second micro-batch. This process continues, creating a ‚Äúpipeline‚Äù of operations across devices.\n\n\nParallel Computation: After the initial fill time (the time it takes to get all segments working on something), each device is continuously working on different micro-batches at different stages of processing (forward pass, backward pass). This parallelism increases the utilization of each device and speeds up the training process.\n\n\nChallenges and Considerations\n\n\nCommunication Overhead: The need to pass data between devices can introduce communication overhead, especially if the devices are not closely interconnected.\n\n\nBalancing Load: It‚Äôs crucial to divide the model into segments in a way that balances the computational load across all devices to avoid bottlenecks.\n\n\nBubble Time: There‚Äôs an inherent inefficiency called ‚Äúbubble time‚Äù or ‚Äúpipeline bubble,‚Äù which is the idle time when some devices are waiting for data to process. Optimizing the size of micro-batches and the number of segments can help minimize this effect.\n\n\ngRPC Server Side Streaming\n\ngRPC Remote Procedure Call\nUses HTTP 2.0, Protocol Buffers as the interface description language, and provides features such as authentication, load balancing, and more\n\nServer-side Streaming RPC\nWhat\n\nClient sends a single request to server and receives a stream of responses\n\nHow it works?\n\n\nClient Call: The client initiates the RPC call by sending a single request to the server.\n\n\nStream Initialization: Upon receiving the request, the server starts processing and initializes a stream of responses.\n\n\nData Streaming: The server then sends multiple response messages back to the client over the established connection. These messages are sent as soon as they are ready, without waiting for all the responses to be generated. This is particularly useful for real-time data streaming or when the full set of responses is not immediately available.\n\n\nCompletion: Once all the response messages have been sent, the server signals the end of the stream to the client. The client then closes the connection.\n\n\nLarge scale training\nresource: www.youtube.com/watch\nData Parallel\nEach worker:\n\nhas a copy of the entire neural network model\nresponsible for compute of a portion of data (training minibatch)\n\n\nCommunication\n\n\nAllreduce\n\nany exposed co\n"},"Large-Model-Systems/Training-GPT-from-Scratch":{"title":"Training GPT from Scratch","links":["ClipNote"],"tags":[],"content":"Source: www.youtube.com/watch\nUsed ClipNote to generate notes mainly for my reference\nReproducing the GPT2 124 Million Parameter Model\nToday‚Äôs task is to reproduce the GPT2 124 million parameter model by understanding its architecture, loading its weights successfully, and generating coherent text with the model. The process involves converting the TensorFlow-based model to PyTorch, examining the model‚Äôs parameters like positional and token embeddings, and ultimately writing a custom GPT2 class to train and potentially surpass the existing model‚Äôs performance.\nUnderstanding the Implementation of GPT2 in PyTorch\nThe encoder in GPT2, known as a decoder-only Transformer, and the cross-attention mechanism are absent in the model. The GPT2 architecture includes reshuffling of layer norms and the addition of an extra layer normalization. The implementation involves defining modules like token embeddings, hidden layers, layer norms, and a final classifier, along with specific details on MLP blocks, non-linearities, and attention operations.\nModel Initialization and Generation Process in GPT-2\nThe provided text explains the process of initializing a GPT-2 model, including handling hyperparameters, creating the model‚Äôs state dictionary, and dealing with transposed weights. It also details the forward function for generating text sequences and the process of setting up and generating text using the model. Additionally, it discusses the differences in text generation results between the manually initialized GPT-2 model and the pre-trained model from Hugging Face.\nAutodetecting and Utilizing Devices in PyTorch\nThe text explains how to autodetect and utilize available devices in PyTorch, such as GPUs or CPUs, for running models efficiently. It demonstrates the process of preparing and tokenizing data sets, and how to create batches for training in Transformers, including handling labels for loss calculation and optimization steps.\nUnderstanding the AtomW Optimizer in PyTorch\nThe Atom Optimizer is an alternative to SGD, with AtomW being a bug fix variant. It utilizes hyperparameters and buffers like m and v, akin to momentum and RMS prop, for individual gradient elements to expedite optimization, particularly beneficial for language models. Implementing AtomW involves careful considerations, such as proper device management and ensuring tensors are appropriately moved for efficient training.\nOptimization Strategies for GPT-2 Training\nThe text discusses the optimization strategy for training the GPT-2 model, focusing on weight tying to save parameters, proper initialization following GPT-2 guidelines, and speeding up training by utilizing lower precision formats like TF32 or FP16 for improved performance on GPUs.\nOptimization Techniques for GPU Memory Bandwidth and Performance\nGPUs benefit from reduced data representation bits for easier data movement and quicker access due to memory bandwidth constraints. Tensor cores in GPUs excel at matrix multiplications, particularly in deep learning tasks, but are often memory-bound. Utilizing tf32 and bf16 precisions internally in operations can significantly speed up computations, although the trade-off involves reduced precision and potential memory bottlenecks.\nThe Transition to BFloat16 and the Impact on Training Efficiency\nThe transition from FP16 to BF16 in training models offers a reduced range but simplifies operations by eliminating the need for gradient scalers. By using torch.AutoCast in PyTorch, only the forward pass and loss calculation are surrounded, maintaining precision. Implementing torch.compile further enhances speed by optimizing operations, reducing memory round trips, and significantly improving processing time, making it a valuable tool for neural network compilation.\nGPU Memory Hierarchy and Flash Attention Optimization\nThe text discusses the memory hierarchy in GPUs, highlighting the role of high bandwidth memory (HBM) and the structure of streaming multiprocessors (SMs) for calculations. It also introduces Flash Attention optimization, a kernel fusion algorithm that significantly improves performance by efficiently utilizing memory hierarchy and avoiding excessive reads and writes to high bandwidth memory. This optimization demonstrates the importance of memory access patterns over floating-point operations and showcases the potential for further enhancements beyond what Torch Compile can provide.\nOptimization Improvements and Hyperparameters in GPT Models\nThe text discusses optimizing a network by increasing computation through adding fake tokens, resulting in a 4% improvement in performance. It also delves into the importance of understanding and adjusting hyperparameters, such as learning rate scheduling, in models like GPT-2 and GPT-3 for enhanced training efficiency and resource utilization.\nOptimizing Training Parameters for GPT-3 Implementation\nThe text discusses setting optimal learning rates for different model sizes, implementing a warm-up and decay strategy, skipping batch size increase complexities, utilizing data sampling without replacement, incorporating weight decay for regularization, and simulating large batch sizes through gradient accumulation in the context of implementing a GPT-3 model.\nUnderstanding Gradient Accumulation\nThe text explains the concept of gradient accumulation in the context of optimizing neural networks. It discusses the need for normalizing loss in the gradient accumulation process to ensure consistent gradients. The narrative also delves into utilizing multiple GPUs for collaborative optimization using distributed data parallelism in PyTorch.\nUnderstanding Distributed Data Parallel (DDP) in Multi-Process Execution\nThe text explains the intricacies of utilizing Distributed Data Parallel (DDP) in a multi-process setting, where eight copies of a process run in parallel with different ranks. It delves into adjusting calculations for multiple processes and ensuring proper synchronization. The discussion covers aspects like data loading, model creation, gradient synchronization, and handling loss calculations within the DDP framework.\nMulti-GPU Training and Data Sets for Language Models\nThe text discusses implementing multi-GPU training for deep learning models, specifically focusing on averaging across ranks and optimizing for speed. It also delves into the datasets used by GPT-2 and GPT-3, detailing the sources like Reddit outbound links and Common Crawl. Furthermore, it introduces the Fine Web dataset for educational content and the process of loading and pre-processing data shards for training a language model.\nTraining progress and evaluation strategies\nThe text details a training process where the GPU‚Äôs capacity allows for a significant batch size, potentially leading to fast and effective training comparable to GPT-2. Additionally, the discussion covers the implementation of evaluation on the validation split and the introduction of H swag evaluation, a multiple-choice dataset for language models, offering early signals and smooth evaluation.\nImplementing HSwag Evaluation in Training Script\nThe text discusses the implementation of HSwag evaluation in a training script by iterating through examples, predicting the option with the lowest loss, synchronizing statistics among processes, and evaluating the HSwag accuracy. The author showcases the progress made in HSwag accuracy, surpassing the performance of GPT2 124M model with only 10 billion tokens in training. Some considerations are shared regarding data distribution, potential influences from the training set, and data quality improvements affecting the accuracy achieved.\nOverview of GPT2 and GPT3 Training\nThe text discusses the optimization process of training GPT2 and GPT3 models, highlighting the improvements achieved in training efficiency and potential adjustments in hyperparameters for better performance. It also mentions the importance of data loader optimization, the possibility of achieving faster training with a higher learning rate, and the comparison between PyTorch and a faster Cuda implementation, LM.C, for training GPT models.\nKey Points Covered\n\n\nReproducing the GPT2 124 Million Parameter Model\n\n\nUnderstanding the Implementation of GPT2 in PyTorch\n\n\nModel Initialization and Generation Process in GPT-2\n\n\nAutodetecting and Utilizing Devices in PyTorch\n\n\nUnderstanding the AtomW Optimizer in PyTorch\n\n\nOptimization Strategies for GPT-2 Training\n\n\nOptimization Techniques for GPU Memory Bandwidth and Performance\n\n\nThe Transition to BFloat16 and the Impact on Training Efficiency\n\n\nGPU Memory Hierarchy and Flash Attention Optimization\n\n\nOptimization Improvements and Hyperparameters in GPT Models\n\n\nOptimizing Training Parameters for GPT-3 Implementation\n\n\nUnderstanding Gradient Accumulation\n\n\nUnderstanding Distributed Data Parallel (DDP) in Multi-Process Execution\n\n\nMulti-GPU Training and Data Sets for Language Models\n\n\nTraining progress and evaluation strategies\n\n\nImplementing HSwag Evaluation in Training Script\n\n\nOverview of GPT2 and GPT3 Training\n\n"},"Large-Model-Systems/Transformers":{"title":"Transformers","links":["Large-Model-Systems/queries-(Q),-keys-(K),-and-values-(V)","Large-Model-Systems/Softmax","Large-Model-Systems/How-a-Transformer-works-at-inference-vs-training-time"],"tags":[],"content":"Source: arxiv.org/pdf/1706.03762\nAttention\nScaled Dot-Production Attention\nAttention function is described as mapping a query and a set of key-value pairs to an output. Output is a weighted sum of the values weighting assigned to each values is computated by a compatibility function of the query with the corresponding keys\nDetails:\n\nInputs: The inputs to the attention mechanism are queries (Q), keys (K), and values (V). These are all vectors. In the context of the Transformer, these vectors are usually outputs from the previous layer of the model.\nDot Products of Queries and Keys: The first step in calculating attention is to find the dot products of the query with all keys. This represents a measure of compatibility or similarity, with higher values indicating greater compatibility.\nScaling: Each dot product is scaled by the inverse square root of the dimension of the keys, dk‚Äã‚Äã1‚Äã. This scaling factor helps prevent the dot product values from growing too large in magnitude, which can lead to computational instability due to the Softmax function operating in regions where it has extremely small gradients.\nSoftmax: Next, a softmax function is applied to the scaled dot products. This step converts the scores into probabilities that sum to one. The softmax essentially picks out the highest scores, magnifying their importance.\nOutput: The output is computed as a weighted sum of the values V. Each value is weighted by the softmax score, ensuring that values corresponding to more compatible keys contribute more to the result.\n\nEquation\nAttention(Q,K,V)=softmax(QKT/dk‚Äã‚Äã)V\nExample setup\nFor our example, let‚Äôs consider the simple sentence: ‚ÄúThe quick brown fox jumps.‚Äù\nStep 1: Tokenization\nThe first step in processing this sentence for a model like the Transformer is tokenization. Tokenization is the process of splitting the text into manageable pieces or tokens. Depending on the model setup, this could be words, subwords, or even characters. For simplicity, let‚Äôs assume word-level tokenization here:\n\nTokens: [‚ÄúThe‚Äù, ‚Äúquick‚Äù, ‚Äúbrown‚Äù, ‚Äúfox‚Äù, ‚Äújumps‚Äù]\n\nStep 2: Embedding\nEach token is then converted into a numerical form known as an embedding. These embeddings are typically learned during training and are capable of capturing semantic meanings of the words. Suppose we‚Äôre using an embedding dimension of 4 for simplicity (real models use much larger dimensions like 512). After embedding, each word might be represented as follows:\n\n‚ÄúThe‚Äù: [0.1,0.2,0.3,0.4][0.1,0.2,0.3,0.4]\n‚Äúquick‚Äù: [0.5,0.6,0.7,0.8][0.5,0.6,0.7,0.8]\n‚Äúbrown‚Äù: [0.9,1.0,1.1,1.2][0.9,1.0,1.1,1.2]\n‚Äúfox‚Äù: [1.3,1.4,1.5,1.6][1.3,1.4,1.5,1.6]\n‚Äújumps‚Äù: [1.7,1.8,1.9,2.0][1.7,1.8,1.9,2.0]\n\nStep 3: Generating Queries, Keys, and Values\nIn the Transformer, each token‚Äôs embedding is used to generate queries, keys, and values. This is done through different linear transformations (i.e., different sets of weights). For simplicity, let‚Äôs assume these transformations just reshape the embeddings a bit (in practice, they would be learned matrices). We‚Äôll use simplified transformations:\n\nQueries (Q), Keys (K), and Values (V) might end up looking something like:\n\n‚ÄúThe‚Äù: Q=[0.1,0.2],K=[0.2,0.1],V=[0.3,0.4]\n‚Äúquick‚Äù: Q=[0.5,0.6],K=[0.6,0.5],V=[0.7,0.8]\n‚Äúbrown‚Äù: Q=[0.9,1.0],K=[1.0,0.9],V=[1.1,1.2]\n‚Äúfox‚Äù: Q=[1.3,1.4],K=[1.4,1.3],V=[1.5,1.6]\n‚Äújumps‚Äù: Q=[1.7,1.8],K=[1.8,1.7],V=[1.9,2.0]\n\n\n\nStep 4: Calculating Attention for One Word\nLet‚Äôs focus on calculating the attention for the word ‚Äúquick‚Äù:\n\nCompute dot products of ‚Äúquick‚Äù query with all keys to measure compatibility.\nApply a scaling factor and softmax to these scores to get probabilities.\nUse these probabilities to compute a weighted sum of the values, which gives you the attention output for ‚Äúquick‚Äù.\n\nVisualization:\nThis process essentially allows the model to ‚Äúattend‚Äù to all words in the sentence when processing the word ‚Äúquick‚Äù, but to varying degrees based on how relevant each word is to ‚Äúquick‚Äù (as determined by the softmax scores of their dot products).\nPractical application in transformer\nIn the Transformer, this attention mechanism is used in three different ways:\n\nEncoder self-attention: Each position in the encoder can attend to all positions in the previous layer of the encoder.\nDecoder self-attention: Each position in the decoder can attend to all positions up to and including that position in the decoder, using masking to preserve causality.\nEncoder-decoder attention: Queries from the decoder attend to all positions in the encoder.\n\nHow a Transformer works at inference vs training time\n\nsource: www.youtube.com/watch\n\n Inouts ‚Üí Input Embeddings yo\n"},"Large-Model-Systems/VIdeo-understanding":{"title":"VIdeo understanding","links":[],"tags":[],"content":"\naccomplished by encoding the video as a sequence of frames in the large context window.\n"},"Large-Model-Systems/Vector-matrix-multiplication-(VMM)":{"title":"Vector-matrix multiplication (VMM)","links":[],"tags":[],"content":"Vector-matrix multiplication is an operation in linear algebra where a vector is multiplied by a matrix. Here‚Äôs how it works:\n\nVector Definition: A vector is a column or row of numbers. For example, a column vector ( \\mathbf{v} ) might look like:\n\n   \\[\n   \\mathbf{v} = \\begin{bmatrix}\n   v_1 \\\\\n   v_2 \\\\\n   v_3\n   \\end{bmatrix}\n   \\]\n\nMatrix Definition: A matrix is a rectangular array of numbers arranged in rows and columns. For example, a matrix ( \\mathbf{M} ) might look like:\n\n   \\[\n   \\mathbf{M} = \\begin{bmatrix}\n   m_{11} &amp; m_{12} &amp; m_{13} \\\\\n   m_{21} &amp; m_{22} &amp; m_{23} \\\\\n   m_{31} &amp; m_{32} &amp; m_{33}\n   \\end{bmatrix}\n   \\]\n\nMultiplication Process: To multiply a vector ( \\mathbf{v} ) by a matrix ( \\mathbf{M} ), the vector must be post-multiplied by the matrix (if the vector is a column vector). The resulting product is another vector where each element is the dot product of the original vector and each row of the matrix.\n\n   \\[\n   \\mathbf{u} = \\mathbf{M} \\mathbf{v}\n   \\]\nFor example, if ( \\mathbf{v} ) is a 3-dimensional column vector and ( \\mathbf{M} ) is a ( 3 \\times 3 ) matrix, the resulting vector ( \\mathbf{u} ) will be calculated as follows:\n\\[\n\\mathbf{u} = \\begin{bmatrix}\nm_{11} &amp; m_{12} &amp; m_{13} \\\\\nm_{21} &amp; m_{22} &amp; m_{23} \\\\\nm_{31} &amp; m_{32} &amp; m_{33}\n\\end{bmatrix}\n\\begin{bmatrix}\nv_1 \\\\\nv_2 \\\\\nv_3\n\\end{bmatrix}\n\\]\nEach element ( u_i ) of the resulting vector ( \\mathbf{u} ) is computed as:\n\\[\nu_i = \\sum_{j=1}^{n} m_{ij} v_j\n\\]\nWhere ( n ) is the number of elements in the vector ( \\mathbf{v} ).\nFor example, if:\n\\[\n\\mathbf{M} = \\begin{bmatrix}\n1 &amp; 2 &amp; 3 \\\\\n4 &amp; 5 &amp; 6 \\\\\n7 &amp; 8 &amp; 9\n\\end{bmatrix}\n, \\quad \\mathbf{v} = \\begin{bmatrix}\nv_1 \\\\\nv_2 \\\\\nv_3\n\\end{bmatrix}\n\\]\nThen the result of ( \\mathbf{M} \\mathbf{v} ) is:\n\\[\n\\mathbf{u} = \\begin{bmatrix}\n1 \\cdot v_1 + 2 \\cdot v_2 + 3 \\cdot v_3 \\\\\n4 \\cdot v_1 + 5 \\cdot v_2 + 6 \\cdot v_3 \\\\\n7 \\cdot v_1 + 8 \\cdot v_2 + 9 \\cdot v_3\n\\end{bmatrix}\n\\]\nIn general, vector-matrix multiplication is fundamental in many applications, including computer graphics, engineering, physics, and machine learning."},"Large-Model-Systems/Vision-Transformer":{"title":"Vision Transformer","links":[],"tags":[],"content":"fooba"},"Large-Model-Systems/index":{"title":"Large Model Systems","links":["Large-Model-Systems/Torchserve","Large-Model-Systems/LLaVA","Large-Model-Systems/1-bit-LLMs","Large-Model-Systems/Gemini","Large-Model-Systems/Transformers","Large-Model-Systems/Quantization","Large-Model-Systems/MatMul-Free-Language-Modeling","Large-Model-Systems/LlaMa-from-scratch","Large-Model-Systems/Liger-Kernels"],"tags":[],"content":"Deeply examining how large models (GPT-4, Llama-2, etc) are trained and deployed\nList of following systems in the notes:\n\nTorchserve\nLLaVA\n1 bit LLMs\nGemini\n\nList of topics:\n\nTransformers\nQuantization\nMatMul-Free Language Modeling\nLlaMa from scratch\n\n\nLiger Kernels\n\n"},"Large-Model-Systems/queries-(Q),-keys-(K),-and-values-(V)":{"title":"queries (Q), keys (K), and values (V)","links":[],"tags":[],"content":"\n\nQuery: A query is a representation of an input element that is used to score against all keys. It can be thought of as a request or a set of features that seek to retrieve information from the values.\n\n\nKeys: Keys are representations of all input elements that are compared against the query. The comparison is used to determine the degree of ‚Äúattention‚Äù or relevance of each element in the values.\n\n\nValues: Values are the representations of all input elements that contain the actual information that needs to be retrieved. The amount of information pulled from each value is weighted by the computed scores from the query-key comparison.\n\n"},"Links":{"title":"Links","links":["calendly.com/viraat","viraat@inferent.io","github.com/viraatdas"],"tags":[],"content":"Contact me\n\nCalendly\nEmail: viraat@inferent.io\n\nProjects\n\nGitHub\nResearchGate\n\nSocials\n\nX / Twitter\nMovies / Shows\n"},"Linux-Kernel-Development/index":{"title":"Linux Kernel Development","links":[],"tags":[],"content":""},"Linux-Kernel-Development/linux_kernel_development":{"title":"linux_kernel_development","links":[],"tags":[],"content":"Chapter 3\n\nfork() is implemented through Copy-on-write (COW)\n\ntechnique to delay or altogether prevent copying of the data.\nThe parent and child can share a single copy of the address space\nIf it‚Äôs written to, a duplicate is made and each process receives a unique copy\n\n\nvfork() - child executing in a new address space ‚Üí the child executes as the sole thread in the parent‚Äôs address space and parent is blocked untilt the child either calls exec() or exits\nLookin into ptracing and why that is beneficial\n\nChapter 4 - Processor Scheduling\n\nMultitasking operating systems\n\ncooperative\n\nact of process voluntarily suspending itself is called yielding\nMac OS 9 (and earlier) and Windows 3.1 (and earlier) employed cooperative multitsking\n\n\npreemptive\n\nLinux like other Unix variants and most modern OS implements this\ntimeslice - time a process runs is predetermined\n\n\n\n\nDuring 2.5 kernel development series, the Linux kernel received a scheduler overhaul\n\nnew scheduler called the O(1) scheduler solve the shortcomings of the previous Linux scheduler\n\nconstant-time algorithm for timeslice calculating and per-processor runqueues\nIt. had issues with latency-sensitive applications. These applications were called interactive processes which includes any application with which the user interactions\nIt performed well with large server workfload - whcih lack interactive processes - it perform below par on desktop dystem, where interactive applications are the raison d‚Äô√™tre\n\n\n\n\n2.6 kernel series introduced new process schedulers - most notable was the Rotating Straicase Deadline scheduler - introduced the concept of fair scheduling\n\nthis replace the O(1) scheduler replacement in 2.6.23, the Completely Fair Scheduler, CFS\n\n\n\nPolicy\nbehavior of scheduler that determines what runs when.\nI/O-Bound versus processor-bound processes\n\nI/O-bound\n\nI/O means any type of blockable resource such as keyboard input or network I/O, not just disk I/O\nthis is a process that psneds much of its time submitting and waiting on I/O requests\nbecause of this, it‚Äôs only runnable for short durations\nMost GUI aplications are this - because they spend most their time waiting on user interactive via the keyboard and mouse\n\n\nprocess-bound\n\nspend much of their time executing code\nexample an infinite loop or lot of mathematicaly calculations like MATLAB or ssh-keygen\n\n\nProcess can exhibit both behaviors\n\nX window Server is both\nWord processor where it waits for key presses but then might jump into rapid fit of spell checking or macro calculation\n\n\nscheduling policy must attempt to satisfy two conflicting goals: fast process response time (low latency) and maximal system utilization time (high throughput)\n\nLinux and Unix tends to favor I/O bound processes over processor bound processes\n\n\n\nProcess Priority\n\ncommon type of scheduling algorithm is priority-based scheduling\n\nrank processed based on their worht and need for processor time\nprocesses with a higher priority run before those with a lower priority\nsame pritiy are scheduled round-robin\nOn some systems, processes with a higher priotiy also receive a longer timeslice\n\n\nLinux kernel implements two separate priority ranges\n\nnice value\n\na number from -20 to +19 with a default of 0.\nlarger nice values correspond to lower priotiy - you are being ‚Äúnice‚Äù to the other processes on the system\nProcesses with a lower nice value (higher priority) receive a larger proportion of the system‚Äôs processor\nNice values are the stadnard priority range used in all Unix system\nMac OS X, the nice value is a control over the absolute timeslice allotated to a process\nin linux it is a control over the proportion of timeslice\n\n\nreal-time priority\n\nby default range from 0 to 99 inclusive\nLinux implementes real-time priroity in accordance with the relevant Unix standards, specifically POSIX.1b\nhigher real-time priority values correspodn to a greater priority\n\n\n\n\n\nTimeslice (quantum or processor slice)\n\nnumeric value that represents how long a task can run until it is preempted\nIn Linux is a proprotion assigned based on the nice value\nIn Linux, under the new CFS scheduler, the decision is a function of how much of a proportion of the processor the newly runnable processor has consumed.\n\nScheduling Policy in Action\n\nSystem with two runnable tasks: text editor and a video encoder\ntext editor is I/O bound and video processor\nLets say they have the same nice value, they get allotted 50% each\nLinux guarantees half of the processor time.\n\nLinux Scheduling Algorithm\nScheduler classes\nLinux is modular enabling different algorithms to schedule different types of processes. This modularity is called scheduler classes.\nCFS is registered scheduler class for normal processes, called SCHED_NORMAL in Linux (and SCHED_OTHER in POSIX).\nProcessing Scheduling in Unix systems\n\npriority\n\nprocess with higher priority run more frequently and receive a higher timeslice (typically)\n\n\ntimeslice\n\nhow long a process runs; they start with some default timeslice\n\n\nUnix the priority is exported to user-space in the form of nice values. This leads to a lot of problems though.\n\nmapping nice values onto timeslices requires a decision about what absolute timeslot to allot each nice value. This leads to suboptimal switching behavior.\nRelative nice values and again the nice value to timeslice mapping.\n\n\nReread timeslice mapping chapter after Chapter 11\n"},"MRI/index":{"title":"index","links":[],"tags":[],"content":"B0‚Äã"},"Memory/Memory":{"title":"Memory","links":["resources/memory-lwn.pdf"],"tags":[],"content":"main resource: memory-lwn.pdf\nIntroduction"},"Memory/index":{"title":"Memory","links":[],"tags":[],"content":""},"Political-Theory/Political-Theory":{"title":"Political Theory","links":["Social-classes"],"tags":[],"content":"Source: George H. Sabine - A History of Political Theory\nChapter 1\n\nmodern political ideals are based on Greek thinkers upon the institutions of the city-state\n\nSocial classes\n\nbottom of the social scale were slaves (third were slaves in Athens)\nGreeks did not have a huge leisure class\nMost Athenian citizens must have been tradesman or artistans or farmers\nAristotle wanted all the manual work to be done by slaves in order that citizens might have leisure to devote themselves to politics\n"},"Political-Theory/index":{"title":"Political Theory","links":[],"tags":[],"content":""},"Psychology/Developmental-Psychology":{"title":"Developmental Psychology","links":["Psychology/Piaget","Psychology/Erickson","Psychology/Vygotsky","Erikson"],"tags":[],"content":"source: David G Myers Psychology book\nMajor Theories of Development:\n1. Piaget stages of Cognitive Development\n2. Erickson‚Äôs Psychosocial Stages\n3. Vygotsky‚Äôs Sociocultural Theory\n\nPiaget: Focus on how children think and how their thinking changes over time.\nErikson: Emphasizes the social and emotional aspects of development across the lifespan.\nVygotsky: Highlights the role of social interaction and cultural context in development.\n"},"Psychology/Erickson":{"title":"Erickson","links":[],"tags":[],"content":"\nTrust vs. Mistrust (0-1 year): Developing trust when caregivers provide reliability, care, and affection.\nAutonomy vs. Shame and Doubt (1-3 years): Developing a sense of personal control over physical skills and a sense of independence.\nInitiative vs. Guilt (3-6 years): Asserting control and power over the environment, leading to a sense of purpose.\nIndustry vs. Inferiority (6-12 years): Coping with new social and academic demands, leading to a sense of competence.\nIdentity vs. Role Confusion (12-18 years): Developing a personal identity and sense of self.\nIntimacy vs. Isolation (young adulthood): Forming intimate relationships with others.\nGenerativity vs. Stagnation (middle adulthood): Contributing to society and helping to guide future generations.\nIntegrity vs. Despair (late adulthood): Reflecting on life and feeling a sense of fulfillment or regret.\n"},"Psychology/Piaget":{"title":"Piaget","links":["Object-permanence","Symbolic-play","egocentrism","Conservation","reversibility","Abstract-logic"],"tags":[],"content":"\nSensorimotor Stage (0-2 years): Understanding the world through sensory experiences and physical actions. Key concept: Object permanence.\nPreoperational Stage (2-7 years): Use of symbols, language development, egocentric thinking. Key concepts: Symbolic play, egocentrism.\nConcrete Operational Stage (7-11 years): Logical thinking about concrete events, understanding of conservation. Key concepts: Conservation, reversibility.\nFormal Operational Stage (12+ years): Abstract and hypothetical thinking. Key concept: Abstract logic.\n"},"Psychology/Vygotsky":{"title":"Vygotsky","links":[],"tags":[],"content":"\nZone of Proximal Development (ZPD): The difference between what a learner can do without help and what they can achieve with guidance.\nScaffolding: Supportive activities provided by a more knowledgeable other to help the learner achieve the next level of understanding.\n"},"Psychology/index":{"title":"Psychology","links":["Psychology/Developmental-Psychology"],"tags":[],"content":"\nDevelopmental Psychology\n\n"},"Religion/Bible/Bible":{"title":"Bible","links":["Religion/Bible/Genesis/Genesis"],"tags":[],"content":"source: New Living Translation Bible\n\nGenesis\n"},"Religion/Bible/Genesis/Genesis-1-2":{"title":"Genesis 1-2","links":["Creation","Sabbath","Humanity"],"tags":[],"content":"Genesis 1-2: Creation\nSummary\n\nDay 1: Light\nDay 2: Sky and Water\nDay 3: Land and Vegetation\nDay 4: Sun, Moon, and Stars\nDay 5: Sea Creatures and Birds\nDay 6: Animals and Humans\nDay 7: Rest\n\nKey Themes\n\nCreation\nSabbath\nHumanity\n\nImportant Verses\n\nGenesis 1:1 - ‚ÄúIn the beginning, God created the heavens and the earth.‚Äù\nGenesis 1:27 - ‚ÄúSo God created man in his own image.‚Äù\n"},"Religion/Bible/Genesis/Genesis-10-11":{"title":"Genesis 10-11","links":["Diversity","Pride"],"tags":[],"content":"Genesis 10-11: Nations and Babel\nSummary\n\nTable of Nations\nTower of Babel\n\nKey Themes\n\nDiversity\nPride\n\nImportant Verses\n\nGenesis 11:4 - ‚ÄúLet us build ourselves a city, with a tower that reaches to the heavens‚Ä¶‚Äù\n"},"Religion/Bible/Genesis/Genesis-12-25":{"title":"Genesis 12-25","links":["Faith","Promise","Testing"],"tags":[],"content":"Genesis 12-25: Abraham\nSummary\n\nCall of Abram\nCovenant with Abraham\nSodom and Gomorrah\nSacrifice of Isaac\n\nKey Themes\n\nFaith\nPromise\nTesting\n\nImportant Verses\n\nGenesis 12:1 - ‚ÄúGo from your country, your people and your father‚Äôs household to the land I will show you.‚Äù\nGenesis 22:2 - ‚ÄúTake your son, your only son, whom you love‚ÄîIsaac‚Ä¶‚Äù\n"},"Religion/Bible/Genesis/Genesis-26-27":{"title":"Genesis 26-27","links":["Inheritance","Deception"],"tags":[],"content":"Genesis 26-27: Isaac\nSummary\n\nIsaac and Rebekah\nIsaac‚Äôs dealings with Abimelek\nBlessing of Jacob over Esau\n\nKey Themes\n\nInheritance\nDeception\n\nImportant Verses\n\nGenesis 27:28 - ‚ÄúMay God give you heaven‚Äôs dew and earth‚Äôs richness‚Ä¶‚Äù\n"},"Religion/Bible/Genesis/Genesis-28-36":{"title":"Genesis 28-36","links":["Transformation","Perseverance"],"tags":[],"content":"Genesis 28-36: Jacob\nSummary\n\nJacob‚Äôs Dream\nMarriage to Leah and Rachel\nJacob and Esau Reconcile\n\nKey Themes\n\nTransformation\nPerseverance\n\nImportant Verses\n\nGenesis 28:15 - ‚ÄúI am with you and will watch over you wherever you go‚Ä¶‚Äù\nGenesis 32:28 - ‚ÄúYour name will no longer be Jacob, but Israel‚Ä¶‚Äù\n"},"Religion/Bible/Genesis/Genesis-3":{"title":"Genesis 3","links":["Sin","Temptation","Redemption"],"tags":[],"content":"Genesis 3: The Fall\nSummary\n\nTemptation by the serpent\nDisobedience and eating the forbidden fruit\nConsequences and expulsion from Eden\n\nKey Themes\n\nSin\nTemptation\nRedemption\n\nImportant Verses\n\nGenesis 3:6 - ‚ÄúWhen the woman saw that the fruit of the tree was good for food and pleasing to the eye‚Ä¶‚Äù\nGenesis 3:15 - ‚ÄúAnd I will put enmity between you and the woman‚Ä¶‚Äù\n"},"Religion/Bible/Genesis/Genesis-37-50":{"title":"Genesis 37-50","links":["Forgiveness","Providence"],"tags":[],"content":"Genesis 37-50: Joseph\nSummary\n\nJoseph‚Äôs Dreams\nSold into Slavery\nRise to Power in Egypt\nReunion with Family\n\nKey Themes\n\nForgiveness\nProvidence\n\nImportant Verses\n\nGenesis 50:20 - ‚ÄúYou intended to harm me, but God intended it for good‚Ä¶‚Äù\n"},"Religion/Bible/Genesis/Genesis-4-5":{"title":"Genesis 4-5","links":["Fratricide","Genealogy"],"tags":[],"content":"Genesis 4-5: Early Humanity\nSummary\n\nCain and Abel\nGenerations from Adam to Noah\n\nKey Themes\n\nFratricide\nGenealogy\n\nImportant Verses\n\nGenesis 4:9 - ‚ÄúAm I my brother‚Äôs keeper?‚Äù\nGenesis 5:24 - ‚ÄúEnoch walked faithfully with God; then he was no more, because God took him away.‚Äù\n"},"Religion/Bible/Genesis/Genesis-6-9":{"title":"Genesis 6-9","links":["Judgment","Covenant","Renewal"],"tags":[],"content":"Genesis 6-9: The Flood\nSummary\n\nHumanity‚Äôs wickedness\nNoah‚Äôs Ark\nThe Great Flood\nCovenant with Noah\n\nKey Themes\n\nJudgment\nCovenant\nRenewal\n\nImportant Verses\n\nGenesis 6:8 - ‚ÄúBut Noah found favor in the eyes of the Lord.‚Äù\nGenesis 9:13 - ‚ÄúI have set my rainbow in the clouds‚Ä¶‚Äù\n"},"Religion/Bible/Genesis/Genesis":{"title":"Genesis","links":["Religion/Bible/Genesis/Genesis-1-2","Religion/Bible/Genesis/Genesis-3","Religion/Bible/Genesis/Genesis-4-5","Religion/Bible/Genesis/Genesis-6-9","Religion/Bible/Genesis/Genesis-10-11","Religion/Bible/Genesis/Genesis-12-25","Religion/Bible/Genesis/Genesis-26-27","Religion/Bible/Genesis/Genesis-28-36","Religion/Bible/Genesis/Genesis-37-50"],"tags":[],"content":"Genesis Overview\nSummary\nGenesis is the first book of the Bible, detailing the creation of the world, the early history of humanity, and the origins of the Israelites.\nMajor Themes\n\nCreation\nFall of Man\nFlood\nPatriarchs (Abraham, Isaac, Jacob)\nJoseph‚Äôs story\n\nKey Sections\n\nCreation\nThe Fall\nEarly Humanity\nThe Flood\nNations and Babel\nAbraham\nIsaac\nJacob\nJoseph\n"},"Religion/index":{"title":"Religion","links":[],"tags":[],"content":""},"Screenplay-Writing/Screenplay-Course":{"title":"Screenplay Course","links":[],"tags":[],"content":"www.coursera.org/learn/write-a-feature-length-screenplay-for-film-or-television/lecture/xMKiP/creating-the-idea-for-your-feature-film\nWeek 1: Creating the idea for your feature film\nDeveloping the idea"},"Screenplay-Writing/index":{"title":"Screenplay Writing","links":[],"tags":[],"content":""},"Verilog/index":{"title":"Verilog","links":[],"tags":[],"content":""},"Verilog/learning_verilog":{"title":"learning_verilog","links":[],"tags":[],"content":"Verilog\nwww.youtube.com/watch\nVerilog Modelling Styles\nCan be done in three different abstractions:\n\nGate level [less abstract]\nDataflow level [more abstract]\nBehavioral level [even more abstract]\n\nCombinational Logic vs Sequential Logic\nCombinational Logic\nOutput is a function of the input\n\n2 to 1 multiplexer\n\n\nGate level\nDescribes the actual gate and how they are connected\n\n\n\nDataflow level\nDescribes the flow of data in the circuit\n\n\n\nBehavioral level\ndescribes the behavior, rather than what the circuit is\n\n\n\nSequential Logic\nRequires:\n\nmemory\nstate\n"},"index":{"title":"Content Feed","links":[],"tags":[],"content":"Curious what Viraat is consuming? No? Still consume and become smarter.\nJune 7, 2024\n\nKhalistan Explained\n\nJune 6, 2024\n\nI Sent My Lookalike On NATIONAL News\n\nMay 22, 2024\n\nWhen a director breaks all of Pixar‚Äôs rules\n\nApril 24, 2024\n\nThe Problem With Interstellar‚Äôs Black Hole that Everyone Ignores\nI accidentally killed this magnet\n\nMarch 26, 2024\n\nSomething Fascinating About Traffic Signals\nAll Three Holes\nColorado is not a perfect rectangle\n\nMarch 1, 2024\n\nHackMIT 2019 Puzzle 6\n\nFebruary 23, 2024\n\nAncient Origins of the Chinese Triads\n\nFebruary 21, 2024\n\nBrain embeddings to artificial context\nGPT tokenizer explained\nI Hacked Into My Own Car\nTechnical accessibility in programming\nWriting musically\n\nFebruary 20, 2024\n\nDoing math is lonely\n\nFebruary 17, 2024\n\nThe best way to start learning Verilog\n\nFebruary 14, 2024\n\nHow to Start a War With a Flash Drive\n\nFebruary 6, 2024\n\nDIY Battery Testing Robot Part Two\nHow do you defend someone you think is guilty\nWhat is an FPGA and how to program it\nNVDLA Primer Documentation\n\nFebruary 5, 2024\n\n19 year old crossing the Pacific alone\nHow I take notes in mathematics lectures using LaTeX and Vim\n\nFebruary 4, 2024\n\nHow petrol pumps know when to turn themselves off\n\nJanuary 29, 2024\n\nThe Rise and Fall of Somali Pirates\n\nJanuary 28, 2024\n\nStriking A Match With a Bullet (380,117 fps SlowMo)\nI Pranked America‚Äôs Most Racist Man\n\nJanuary 27, 2024\n\nMaking coffee with olive oil\n\nJanuary 26, 2024\n\nWhy North Korea is so good at propaganda\n\nJanuary 25, 2024\n\nHow Facebook Became a Tool for Genocide\n\nJanuary 24, 2024\n\nWhat G√∂del Discovered\n\nJanuary 20, 2024\n\nFDA Approves First Gene Therapies for Sickle Cell Disease\n\nJanuary 19, 2024\n\nAlphaGeometry: An Olympiad-level AI system for geometry\nI Built a Life-Size Sonic That Can Cut You in Half\nMomo - South Asia‚Äôs Best Dumplings\nWhy light has energy, but no mass\n\nJanuary 18, 2024\n\nHow Benjamin Netanyahu Relies on Hamas\nPolitics and the English Language\nAncient Life as Old as the Universe\nThe unexpectedly hard windmill question\nTracking developer build times\n"}}